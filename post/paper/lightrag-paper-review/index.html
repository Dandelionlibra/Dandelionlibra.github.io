<!doctype html><html lang=zh-Hant-TW dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="導讀《LightRAG-Simple and Fast Retrieval-Augmented Generation》論文，解析其結合知識圖譜與雙層檢索策略的高效 RAG 架構設計與應用。"><title>LightRAG 論文導讀 — Simple and Fast Retrieval-Augmented Generation 筆記</title><link rel=canonical href=https://Dandelionlibra.github.io/post/paper/lightrag-paper-review/><link rel=stylesheet href=/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css><meta property='og:title' content="LightRAG 論文導讀 — Simple and Fast Retrieval-Augmented Generation 筆記"><meta property='og:description' content="導讀《LightRAG-Simple and Fast Retrieval-Augmented Generation》論文，解析其結合知識圖譜與雙層檢索策略的高效 RAG 架構設計與應用。"><meta property='og:url' content='https://Dandelionlibra.github.io/post/paper/lightrag-paper-review/'><meta property='og:site_name' content='YuChen'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='AI'><meta property='article:tag' content='Retrieval-Augmented Generation'><meta property='article:tag' content='RAG'><meta property='article:tag' content='Graph RAG'><meta property='article:tag' content='LightRAG'><meta property='article:tag' content='Knowledge Graph'><meta property='article:tag' content='Research Summary'><meta property='article:published_time' content='2025-07-22T05:27:00+08:00'><meta property='article:modified_time' content='2025-07-22T05:27:00+08:00'><meta name=twitter:title content="LightRAG 論文導讀 — Simple and Fast Retrieval-Augmented Generation 筆記"><meta name=twitter:description content="導讀《LightRAG-Simple and Fast Retrieval-Augmented Generation》論文，解析其結合知識圖譜與雙層檢索策略的高效 RAG 架構設計與應用。"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column compact"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_a0443a4a03d93adb.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>YuChen</a></h1><h2 class=site-description></h2></div></header><ol class=menu-social><li><a href=https://github.com/Dandelionlibra target=_blank title=Github rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://x.com/ target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/paper/>Paper</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/post/paper/lightrag-paper-review/>LightRAG 論文導讀 — Simple and Fast Retrieval-Augmented Generation 筆記</a></h2><h3 class=article-subtitle>導讀《LightRAG-Simple and Fast Retrieval-Augmented Generation》論文，解析其結合知識圖譜與雙層檢索策略的高效 RAG 架構設計與應用。</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 22, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>2 minute read</time></div></footer></div></header><section class=article-content><blockquote><p>本文整理自：<a class=link href=https://arxiv.org/abs/2410.05779 target=_blank rel=noopener>LightRAG: Simple and Fast Retrieval-Augmented Generation</a><br>作者：Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang<br>發佈於 arXiv，2024年10月</p></blockquote><h1 id=摘要>摘要</h1><p>RAG 透過整合外部知識來源，提升 LLMs 回應的準確性與上下文相關性，但面臨<strong>過度依賴平面資料表示</strong> (flat data representations)、<strong>上下文感知能力不足</strong> (inadequate contextual awareness)、<strong>導致生成碎片化答案</strong> (fragmented answers)，無法捕捉複雜的相互依賴關係 (inter-dependencies)。<br>LightRAG，提出透過將圖結構 (graph structures) 引入文本的索引 (text indexing) 和檢索 (retrieval) 過程來解決上述問題。</p><hr><h1 id=引言>引言</h1><h2 id=現有-rag-系統的局限性>現有 RAG 系統的局限性</h2><ul><li><p><strong>依賴平面資料表示：</strong> 許多方法依賴於平面資料表示（flat data representations），限制了它們根據實體之間複雜關係來理解和檢索資訊的能力。</p></li><li><p><strong>缺乏上下文感知：</strong> 這些系統通常缺乏維持不同實體及其相互關係之間連貫性所需的上下文感知能力，導致回應可能無法完全解決用戶查詢。</p><blockquote><p>例：考慮用戶提問「電動車的興起如何影響城市空氣品質和大眾運輸基礎設施？」現有 RAG 方法可能檢索到關於電動車、空氣污染和公共交通挑戰的獨立文檔，但難以將這些信息綜合為一個連貫的回應。它們可能無法解釋電動車的普及如何改善空氣品質，進而可能影響公共交通規劃，用戶可能收到一個碎片化的答案，未能充分捕捉這些主題之間複雜的相互依賴關係。</p></blockquote></li></ul><h2 id=lightrag-模型概述>LightRAG 模型概述</h2><p>增強了系統捕捉實體之間複雜相互依賴關係的能力，從而產生更連貫和上下文更豐富的回應。</p><ul><li><p><strong>高效雙層檢索策略：</strong></p><ul><li>低層次檢索（low-level retrieval）： 側重於關於特定實體及其關係的精確資訊。</li><li>高層次檢索（high-level retrieval）： 涵蓋更廣泛的主題和概念。</li><li>優勢： 透過結合詳細和概念性檢索，LightRAG 有效適應多樣化的查詢範圍，確保用戶收到符合其特定需求的相關且全面的回應。</li></ul></li><li><p><strong>圖結構與向量表示的整合：</strong> 透過將圖結構與向量表示整合在一起，本 LightRAG 促進了相關實體和關係的高效檢索，同時透過從所構建的知識圖中獲取相關結構信息，增強了結果的全面性。</p></li></ul><h2 id=本研究在-rag-系統中的關注點>本研究在 RAG 系統中的關注點</h2><ul><li><p><strong>全面信息檢索</strong> (Comprehensive Information Retrieval)： 索引功能 ϕ(⋅) 必須善於提取全局信息，這對於增強模型有效回答查詢的能力至關重要。</p></li><li><p><strong>高效且低成本檢索</strong> (Efficient and Low-Cost Retrieval)： 索引化的資料結構 𝒟^ 必須能夠實現快速且具成本效益的檢索，以有效處理高容量的查詢。</p></li><li><p><strong>快速適應數據變化</strong> (Fast Adaptation to Data Changes)： 能夠迅速有效地調整數據結構以整合來自外部知識庫的新信息，這對於確保系統在不斷變化的信息環境中保持更新和相關性至關重要。</p></li></ul><hr><h1 id=內文>內文</h1><h2 id=lightrag-框架的整體架構>LightRAG 框架的整體架構</h2><p><img src=https://raw.githubusercontent.com/HKUDS/LightRAG/refs/heads/main/README.assets/b2aaf634151b4706892693ffb43d9093.png loading=lazy alt="LightRAG 框架總覽"><br><em>圖 1. LightRAG 框架總覽（取自原論文）</em></p><p>架構如圖 1 所示。</p><p>流程從<strong>原始文本塊</strong>開始，這些文本塊首先透過<strong>基於圖形的文本索引</strong>（Graph-based Text Indexing）階段進行處理，過程包含幾個關鍵子組件：<strong>實體與關係提取</strong>（Entity & Rel Extraction）、<strong>LLM 剖析</strong>（LLM Profiling）和<strong>去重</strong>（Deduplication），最後的輸出是一個用於檢索的<strong>索引圖</strong>（Index Graph）。<br>接著，Query LLM 接收輸入查詢，並從中生成<strong>低層級關鍵字</strong>（Low-level Keys，包括實體和關係）和<strong>高層級關鍵字</strong>（High-level Keys，包括語境和原始文本塊）。這些關鍵字隨後被送入<strong>雙層級檢索範式</strong>（Dual-level Retrieval Paradigm），此範式與「索引圖」和「原始文本塊」互動，以檢索相關資訊。最終，檢索到的資訊被傳回 Query LLM 進行檢索增強的答案生成（Retrieval-Augmented Answer Generation）。  
圖中展示了以「索引圖」作為核心儲存庫，這張圖不僅用來整理新資訊（索引），也用來尋找資訊（檢索），這代表系統不再只是儲存一堆零散的文字片段，而是將知識組織成一個有結構的網路，能更智慧地找出事物之間的關聯。<br>此外，處理查詢的 LLM 在 LightRAG 多次出現，它不只負責生成最終答案，還會參與理解問題、引導系統去尋找相關資訊，並將找到的資料整合起來。</p><hr><h2 id=基於圖形的文本索引>基於圖形的文本索引</h2><p>LightRAG 透過將文件分割成更小、更易於管理的片段來增強檢索系統。這種策略允許快速識別和存取相關資訊，而無需分析整個文件 。隨後，系統利用大型語言模型（LLMs）來識別和提取各種實體（例如，名稱、日期、位置和事件）以及它們之間的關係 。透過這個過程收集到的資訊將用於創建一個全面的知識圖譜，突顯整個文件集合中的連結和見解。</p><p>圖形生成模組正式表示為：</p><ul><li><strong>實體與關係提取 (R(⋅))</strong>：<ul><li>將文檔切分為更小的片段，方便快速檢索與處理。</li><li>使用大型語言模型抽取實體（如人名、地點、事件）以及它們之間的關係，構建知識圖譜。</li></ul></li></ul><ul><li><strong>LLM 剖析以生成鍵值對 (P(⋅))</strong>：為每個實體與關係生成索引鍵（key）與對應摘要文本（value），形成文本鍵值對（K, V）。實體通常採用名稱作為鍵；關係也由關聯實體摘要或主題語形成鍵。</li></ul><ul><li><strong>去重以優化圖操作 (D(⋅))</strong>：合併相同的實體與關係，以減少圖的複雜度，優化後續運算效率。</li></ul><ul><li><p><strong>優勢</strong>：</p><ul><li>全面性理解：透過多跳子圖 (multi-hop subgraphs) 強化對文本中跨關係依賴的理解。</li><li>檢索效率提升：採用鍵值對結構提升查詢精準度與速度，相較於傳統依賴 chunk traversal 方法更具效率。</li></ul></li><li><p><strong>增量更新 (Incremental Knowledge Base Update)</strong>：新文檔插入時，只對該文檔進行索引解析，並與原有圖進行合併，無需重建整個索引，極大降低計算成本並提升更新速度。</p></li></ul><hr><h2 id=雙層次檢索機制-dual-level-retrieval-paradigm>雙層次檢索機制 (Dual-level Retrieval Paradigm)</h2><ol><li><strong>分類查詢類型</strong></li></ol><ul><li>Specific Queries：查找特定實體的資訊。<br>ex. 誰寫了《Pride and Prejudice》？</li><li>Abstract Queries：探索概念性主題。<br>ex. 人工智慧如何影響現代教育？</li></ul><ol start=2><li><strong>查詢關鍵詞抽取 (Keyword Extraction)</strong></li></ol><ul><li>將查詢分為：<ul><li>低階關鍵詞（Low‑level）——聚焦具體實體或關係（例：人名、事件）。</li><li>高階關鍵詞（High‑level）——概括性主題或概念（例：制度變革趨勢）。</li></ul></li><li>低階檢索 (Low‑Level Retrieval)：透過低階鍵精確定位實體與其屬性或鄰近關係。</li><li>高階檢索 (High‑Level Retrieval)：透過高階鍵尋找涵蓋廣泛主題或總覽資訊。</li></ul><ol start=3><li><strong>圖結構與向量結合檢索</strong></li></ol><ul><li>結合知識圖結構與向量表示（vector embeddings），在進行查詢時同時考量局部（local）與全局（global）語義。</li><li>並引入高階相關結構資訊（如一跳鄰居）加強檢索結果的完整性與關聯度。</li></ul><hr><h2 id=檢索增強答案生成>檢索增強答案生成</h2><ol><li><p>使用檢索回來的資料，將所有相關的實體與關係摘要（由 profiling function 生成；P(⋅)）作為 LLM 的上下文輸入。</p></li><li><p>結合查詢與上下文生成回答，將查詢緊接相關資料餵給 LLM，生成上下文合宜、符合需求的回答。</p></li></ol><hr><h2 id=複雜度分析>複雜度分析</h2><ol><li><p>索引階段：對文本進行實體與關係抽取時，需對每個文本片段呼叫一次 LLM，不增加額外系統負擔。</p></li><li><p>檢索階段：相較於傳統高成本的 GraphRAG 社群遍歷，LightRAG 採用向量搜索與圖結構結合方式──只需一次 API 呼叫與少量 token，即完成檢索。</p></li></ol><hr><h1 id=結論重點整理>結論重點整理</h1><ol><li>圖結構索引（Graph-based Indexing）</li></ol><ul><li>以實體與關係為核心建構知識圖，支援去重與增量更新，不必重建全索引。</li></ul><ol start=2><li>雙層檢索（Dual-level Retrieval）</li></ol><ul><li>低階檢索：精確定位實體與細節資訊。</li><li>高階檢索：捕捉抽象主題與全局脈絡。</li><li>兩者結合確保 全面性 + 精準性。</li></ul><ol start=3><li>向量與圖結合（Hybrid Retrieval）</li></ol><ul><li>結合向量相似度與多跳圖結構檢索，兼顧語義相關與結構關聯。</li></ul><ol start=4><li>低成本高效率</li></ol><ul><li>檢索階段僅需一次 API 呼叫、百級 token，較 GraphRAG 大幅節省計算與金錢成本。</li></ul><ol start=5><li>動態適應性</li></ol><ul><li>能即時合併新知識圖節點，適合動態更新的大型知識庫（如法律、醫療、科研）。</li></ul><p><strong>LightRAG vs GraphRAG</strong></p><div class=table-wrapper><table><thead><tr><th>面向</th><th>LightRAG</th><th>GraphRAG</th></tr></thead><tbody><tr><td><strong>索引結構</strong></td><td>基於 <strong>圖結構（Knowledge Graph）+ 向量索引</strong>，以實體與關係為鍵值對（Key-Value）存儲，支援去重與增量更新</td><td>基於 <strong>圖結構社群（Graph Community）</strong>，以社群摘要為檢索單位</td></tr><tr><td><strong>檢索策略</strong></td><td><strong>雙層檢索</strong>：低階（細節）+ 高階（主題）並結合多跳圖檢索與向量相似度</td><td>單層檢索：依社群摘要進行檢索，缺乏細粒度與多層結合</td></tr><tr><td><strong>生成過程</strong></td><td>檢索到的實體與關係摘要直接送入 LLM，結構化輸入更精準</td><td>將社群摘要送入 LLM，依社群內容生成答案</td></tr><tr><td><strong>檢索成本</strong></td><td>一次 API 呼叫</td><td>多次 API 呼叫</td></tr><tr><td><strong>增量更新</strong></td><td>支援 <strong>快速合併更新</strong>，僅更新新文檔的圖索引</td><td>需重建整個社群報告，成本高</td></tr><tr><td><strong>資訊完整性</strong></td><td>低階檢索補足細節，高階檢索抓全局，全面性佳</td><td>依賴社群摘要，可能遺漏細節</td></tr><tr><td><strong>適用場景</strong></td><td>資料頻繁更新、大型知識庫、多層次查詢需求</td><td>資料相對靜態、偏向高層總覽查詢</td></tr></tbody></table></div><hr><hr><h1 id=reference>Reference</h1><ul><li><a class=link href=https://ar5iv.labs.arxiv.org/html/2410.05779 target=_blank rel=noopener>LightRAG: Simple and Fast Retrieval-Augmented Generation-ar5iv 可視化版本</a></li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/ai/>AI</a>
<a href=/tags/retrieval-augmented-generation/>Retrieval-Augmented Generation</a>
<a href=/tags/rag/>RAG</a>
<a href=/tags/graph-rag/>Graph RAG</a>
<a href=/tags/lightrag/>LightRAG</a>
<a href=/tags/knowledge-graph/>Knowledge Graph</a>
<a href=/tags/research-summary/>Research Summary</a></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/post/paper/re2g-paper-review/><div class=article-details><h2 class=article-title>Re2G 論文導讀 — Retrieve, Rerank, Generate 框架解析與應用</h2></div></a></article><article><a href=/post/note/rag-type-compare-note/><div class=article-details><h2 class=article-title>三種 RAG 架構比較與應用解析 — Naive、Advanced、Modular RAG 差異整理</h2></div></a></article><article><a href=/post/paper/rag-llms-survey-note/><div class=article-details><h2 class=article-title>檢索增強大型語言模型綜述 — RA-LLMs 系統性回顧與應用解析</h2></div></a></article><article><a href=/post/langchain/retrieverqa-2/><div class=article-details><h2 class=article-title>LangChain 記憶型檢索問答：《小王子》文本互動實踐</h2></div></a></article><article><a href=/post/langchain/retrieverqa-1/><div class=article-details><h2 class=article-title>使用 Langchain 框架進行檢索提問</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2025 YuChen</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>