{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368e5faf-7fba-4a71-a2b3-65c89b16bda8",
   "metadata": {},
   "source": [
    "## 安裝 LightRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def86d5e-601c-4d9e-a43f-6facb4e8f85a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightrag-hku\n",
      "  Downloading lightrag_hku-1.4.4-py3-none-any.whl.metadata (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from lightrag-hku)\n",
      "  Downloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting configparser (from lightrag-hku)\n",
      "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting dotenv (from lightrag-hku)\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting future (from lightrag-hku)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting nano-vectordb (from lightrag-hku)\n",
      "  Downloading nano_vectordb-0.0.4.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from lightrag-hku) (3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightrag-hku) (1.26.4)\n",
      "Collecting pandas>=2.0.0 (from lightrag-hku)\n",
      "  Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pipmaster (from lightrag-hku)\n",
      "  Downloading pipmaster-0.9.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pydantic (from lightrag-hku)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv (from lightrag-hku)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pyuca (from lightrag-hku)\n",
      "  Downloading pyuca-1.2-py2.py3-none-any.whl.metadata (649 bytes)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightrag-hku) (68.2.2)\n",
      "Collecting tenacity (from lightrag-hku)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken (from lightrag-hku)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting xlsxwriter>=3.1.0 (from lightrag-hku)\n",
      "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->lightrag-hku) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->lightrag-hku) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.0.0->lightrag-hku)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->lightrag-hku)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->lightrag-hku)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->lightrag-hku)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->lightrag-hku) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->lightrag-hku)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->lightrag-hku)\n",
      "  Downloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->lightrag-hku)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->lightrag-hku)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.10/site-packages (from pipmaster->lightrag-hku) (23.2)\n",
      "Collecting ascii_colors>=0.8.0 (from pipmaster->lightrag-hku)\n",
      "  Downloading ascii_colors-0.11.4-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->lightrag-hku)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->lightrag-hku)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic->lightrag-hku)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->lightrag-hku)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->lightrag-hku)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken->lightrag-hku) (2.31.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ascii_colors>=0.8.0->pipmaster->lightrag-hku) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->lightrag-hku) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->lightrag-hku) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->lightrag-hku) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->lightrag-hku) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->lightrag-hku) (2024.2.2)\n",
      "Downloading lightrag_hku-1.4.4-py3-none-any.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nano_vectordb-0.0.4.3-py3-none-any.whl (5.6 kB)\n",
      "Downloading pipmaster-0.9.2-py3-none-any.whl (25 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading pyuca-1.2-py2.py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading ascii_colors-0.11.4-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.6.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.6/241.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyuca, xlsxwriter, tzdata, typing-extensions, tenacity, regex, python-dotenv, propcache, nano-vectordb, future, frozenlist, configparser, async-timeout, ascii_colors, annotated-types, aiohappyeyeballs, typing-inspection, tiktoken, pydantic-core, pipmaster, pandas, multidict, dotenv, aiosignal, yarl, pydantic, aiohttp, lightrag-hku\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 annotated-types-0.7.0 ascii_colors-0.11.4 async-timeout-5.0.1 configparser-7.2.0 dotenv-0.9.9 frozenlist-1.7.0 future-1.0.0 lightrag-hku-1.4.4 multidict-6.6.3 nano-vectordb-0.0.4.3 pandas-2.3.1 pipmaster-0.9.2 propcache-0.3.2 pydantic-2.11.7 pydantic-core-2.33.2 python-dotenv-1.1.1 pyuca-1.2 regex-2024.11.6 tenacity-9.1.2 tiktoken-0.9.0 typing-extensions-4.14.1 typing-inspection-0.4.1 tzdata-2025.2 xlsxwriter-3.2.5 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lightrag-hku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ac46c-79e5-465f-b0b3-9be71d1afa16",
   "metadata": {},
   "source": [
    "## 官方文件\n",
    "https://github.com/HKUDS/LightRAG\n",
    "\n",
    "* **LLM 選擇 (LLM Selection)**:\n",
    "\n",
    "建議使用參數至少為 320 億 (32B) 的大型語言模型。\n",
    "上下文長度 (context length) 應至少為 32KB，推薦使用 64KB。\n",
    "\n",
    "* **嵌入模型 (Embedding Model)**:\n",
    "\n",
    "高效能的嵌入模型對於 RAG 至關重要。\n",
    "我們推薦使用主流的多語言嵌入模型，例如：BAAI/bge-m3 和 text-embedding-3-large。\n",
    "重要提示： 嵌入模型必須在文件索引建立之前確定，並且在文件查詢階段必須使用相同的模型。\n",
    "\n",
    "* **重排序模型配置 (Reranker Model Configuration)**:\n",
    "\n",
    "配置重排序模型 (Reranker) 可以顯著提升 LightRAG 的檢索效能。\n",
    "當啟用重排序模型時，建議將「混合模式」(mix mode) 設定為預設的查詢模式。\n",
    "我們推薦使用主流的重排序模型，例如：BAAI/bge-reranker-v2-m3 或像 Jina 服務提供的模型。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b3400-5c10-4d8d-82d5-7ef2a04a2b5e",
   "metadata": {},
   "source": [
    "Ollama LLM + Ollama Embedding:\n",
    "建立 .env 檔案\n",
    "```e\n",
    "LLM_BINDING=ollama # 使用 Ollama 的 API 協定來與 LLM 通訊\n",
    "LLM_MODEL=mistral-nemo:latest # 指定要使用的 LLM 模型名稱\n",
    "LLM_BINDING_HOST=http://localhost:11434 # 指定了 Ollama 服務的位址和 port 號\n",
    "# LLM_BINDING_API_KEY=your_api_key # 可選參數，用於在 Ollama 需要 API 金鑰認證時\n",
    "###  Ollama Server context length\n",
    "OLLAMA_NUM_CTX=8192 # 設定 LLM 的上下文長度，較長的上下文可以在一次處理中接收和處理更多文本\n",
    "\n",
    "EMBEDDING_BINDING=ollama # 指定 LightRAG 伺服器要綁定的 Embedding 後端類型為 Ollama\n",
    "EMBEDDING_BINDING_HOST=http://localhost:11434\n",
    "EMBEDDING_MODEL=bge-m3:latest # 指定要使用的 Embedding 模型名稱\n",
    "EMBEDDING_DIM=1024\n",
    "# EMBEDDING_BINDING_API_KEY=your_api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a656bbc-cb72-463c-ab43-5fb04477cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_BINDING=ollama\n",
    "LLM_MODEL=gemma3:27b\n",
    "LLM_BINDING_HOST=http://dandelion-ollama-1:11434\n",
    "# LLM_BINDING_API_KEY=your_api_key\n",
    "###  Ollama Server context length\n",
    "OLLAMA_NUM_CTX=12283 # min=8192 , max=16384\n",
    "\n",
    "EMBEDDING_BINDING=ollama\n",
    "EMBEDDING_BINDING_HOST=http://dandelion-ollama-1:11434\n",
    "EMBEDDING_MODEL=bge-m3:latest\n",
    "EMBEDDING_DIM=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55fd265-24b4-46bf-b4bd-b218d1c275d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00699e-4e2f-4bad-a969-00b2fad748c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79fbcbaa-992a-4cfc-98ed-5dd22393eba1",
   "metadata": {},
   "source": [
    "rerank 整合指南：  \n",
    "https://github.com/HKUDS/LightRAG/blob/main/docs/rerank_integration.md  \n",
    "\n",
    "設置 enable_rerank ＝ True，針對每個查詢控制重新排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424dd14c-e757-49b3-b659-00d6ba9b1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.rerank import custom_rerank, RerankModel\n",
    "\n",
    "# Method 1: Using a custom rerank function with all settings included\n",
    "async def my_rerank_func(query: str, documents: list, top_n: int = None, **kwargs):\n",
    "    return await custom_rerank(\n",
    "        query=query,\n",
    "        documents=documents,\n",
    "        model=\"BAAI/bge-reranker-v2-m3\",\n",
    "        base_url=\"https://api.your-provider.com/v1/rerank\",\n",
    "        api_key=\"your_api_key_here\",\n",
    "        top_n=top_n or 10,  # Handle top_n within the function\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=\"./rag_storage\",\n",
    "    llm_model_func=your_llm_func,\n",
    "    embedding_func=your_embedding_func,\n",
    "    rerank_model_func=my_rerank_func,  # Configure rerank function\n",
    ")\n",
    "\n",
    "# Query with rerank enabled (default)\n",
    "result = await rag.aquery(\n",
    "    \"your query\",\n",
    "    param=QueryParam(enable_rerank=True)  # Control rerank per query\n",
    ")\n",
    "\n",
    "# Query with rerank disabled\n",
    "result = await rag.aquery(\n",
    "    \"your query\",\n",
    "    param=QueryParam(enable_rerank=False)\n",
    ")\n",
    "\n",
    "# Method 2: Using RerankModel wrapper\n",
    "rerank_model = RerankModel(\n",
    "    rerank_func=custom_rerank,\n",
    "    kwargs={\n",
    "        \"model\": \"BAAI/bge-reranker-v2-m3\",\n",
    "        \"base_url\": \"https://api.your-provider.com/v1/rerank\",\n",
    "        \"api_key\": \"your_api_key_here\",\n",
    "    }\n",
    ")\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=\"./rag_storage\",\n",
    "    llm_model_func=your_llm_func,\n",
    "    embedding_func=your_embedding_func,\n",
    "    rerank_model_func=rerank_model.rerank,\n",
    ")\n",
    "\n",
    "# Control rerank per query\n",
    "result = await rag.aquery(\n",
    "    \"your query\",\n",
    "    param=QueryParam(\n",
    "        enable_rerank=True,  # Enable rerank for this query\n",
    "        chunk_top_k=5       # Number of chunks to keep after reranking\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b915b-86a2-49a7-aba5-4d86537588a6",
   "metadata": {},
   "source": [
    "``` python\n",
    "from lightrag.rerank import custom_rerank\n",
    "\n",
    "# Your custom API endpoint\n",
    "result = await custom_rerank(\n",
    "    query=\"your query\",\n",
    "    documents=documents,\n",
    "    model=\"BAAI/bge-reranker-v2-m3\",\n",
    "    base_url=\"https://api.your-provider.com/v1/rerank\",\n",
    "    api_key=\"your_api_key_here\",\n",
    "    top_n=10\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fb4eaf-c3de-47bb-b19a-65fc50e39077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a13fb3c-71a2-43ae-95aa-92e90ff51056",
   "metadata": {},
   "source": [
    "Jina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2297e3da-f1b1-4272-837b-c2ce497d93e2",
   "metadata": {},
   "source": [
    "``` python\n",
    "from lightrag.rerank import jina_rerank\n",
    "\n",
    "result = await jina_rerank(\n",
    "    query=\"your query\",\n",
    "    documents=documents,\n",
    "    model=\"BAAI/bge-reranker-v2-m3\",\n",
    "    api_key=\"your_jina_api_key\",\n",
    "    top_n=10\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d226073-17c4-40b9-8a70-79a88204f2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "def4f52a-7c1a-495a-a34e-3f2cea03fabf",
   "metadata": {},
   "source": [
    "Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2dc5b-257b-4d98-b302-5307bfaf4952",
   "metadata": {},
   "source": [
    "\n",
    "``` python\n",
    "from lightrag.rerank import cohere_rerank\n",
    "\n",
    "result = await cohere_rerank(\n",
    "    query=\"your query\",\n",
    "    documents=documents,\n",
    "    model=\"rerank-english-v2.0\",\n",
    "    api_key=\"your_cohere_api_key\",\n",
    "    top_n=10\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fefe6f0-e0cb-43c0-8cda-0169060f4ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a76d54a5-c564-4d62-a737-ee1ac8723d04",
   "metadata": {},
   "source": [
    "官方程式：  \n",
    "https://github.com/HKUDS/LightRAG/blob/main/examples/rerank_example.py\n",
    "https://github.com/HKUDS/LightRAG/blob/main/lightrag/rerank.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce13565-1595-4b72-8eec-850a0ff5e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LightRAG Rerank Integration Example\n",
    "\n",
    "This example demonstrates how to use rerank functionality with LightRAG\n",
    "to improve retrieval quality across different query modes.\n",
    "\n",
    "Configuration Required:\n",
    "1. Set your LLM API key and base URL in llm_model_func()\n",
    "2. Set your embedding API key and base URL in embedding_func()\n",
    "3. Set your rerank API key and base URL in the rerank configuration\n",
    "4. Or use environment variables (.env file):\n",
    "   - RERANK_MODEL=your_rerank_model\n",
    "   - RERANK_BINDING_HOST=your_rerank_endpoint\n",
    "   - RERANK_BINDING_API_KEY=your_rerank_api_key\n",
    "\n",
    "Note: Rerank is now controlled per query via the 'enable_rerank' parameter (default: True)\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.rerank import custom_rerank, RerankModel\n",
    "# from lightrag.llm.openai import openai_complete_if_cache, openai_embed\n",
    "from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "from lightrag.utils import EmbeddingFunc, setup_logger\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set up your working directory\n",
    "WORKING_DIR = \"/workspace/yuchen/light_test_rerank\"\n",
    "setup_logger(\"test_rerank\")\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "# 設定 Ollama 服務的 URL\n",
    "OLLAMA_BASE_URL = \"http://dandelion-ollama-1:11434\"\n",
    "\n",
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], **kwargs\n",
    ") -> str:\n",
    "    return await ollama_model_complete(\n",
    "        \"qwen2.5:0.5b\",\n",
    "        prompt,\n",
    "        system_prompt=system_prompt,\n",
    "        history_messages=history_messages,\n",
    "        # api_key=\"your_llm_api_key_here\",\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "async def embedding_func(texts: list[str]) -> np.ndarray:\n",
    "    return await ollama_embed(\n",
    "        texts,\n",
    "        embed_model=\"nomic-embed-text:latest\",\n",
    "        # api_key=\"your_embedding_api_key_here\",\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "async def ollama_rerank(\n",
    "    query: str, \n",
    "    documents: list, \n",
    "    model: str = \"xitao/bge-reranker-v2-m3:latest\", \n",
    "    base_url: str = OLLAMA_BASE_URL,\n",
    "    top_n: int = None, \n",
    "    **kwargs\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    ¨使用 Ollama 服務進行 rerank 的自訂函式\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/rerank\"\n",
    "    \n",
    "    # Ollama rerank API »Ý­nªº payload ®æ¦¡\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"query\": query,\n",
    "        \"documents\": [doc[\"content\"] for doc in documents], # Ollama rerank documents °Ñ¼Æ®æ¦¡\n",
    "        \"top_n\": top_n or 10,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        reranked_results = response.json().get(\"results\", [])\n",
    "        \n",
    "        # ±Nµ²ªGÂà´«¦^ LightRAG ªº¤å¥ó®æ¦¡\n",
    "        reranked_docs = []\n",
    "        for result in reranked_results:\n",
    "            original_index = result[\"index\"]\n",
    "            score = result[\"score\"]\n",
    "            doc = documents[original_index].copy()\n",
    "            doc[\"rerank_score\"] = score\n",
    "            reranked_docs.append(doc)\n",
    "            \n",
    "        return reranked_docs\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error calling Ollama rerank API: {e}\")\n",
    "        return []\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "async def my_rerank_func(query: str, documents: list, top_n: int = None, **kwargs):\n",
    "    \"\"\"Custom rerank function with all settings included\"\"\"\n",
    "    return await ollama_rerank(\n",
    "        query=query,\n",
    "        documents=documents,\n",
    "        top_n=top_n or 10,\n",
    "        **kwargs,\n",
    "    )\n",
    "    # return await custom_rerank(\n",
    "    #     query=query,\n",
    "    #     documents=documents,\n",
    "    #     model=\"BAAI/bge-reranker-v2-m3\",\n",
    "    #     base_url=\"https://api.your-rerank-provider.com/v1/rerank\",\n",
    "    #     api_key=\"your_rerank_api_key_here\",\n",
    "    #     top_n=top_n or 10,\n",
    "    #     **kwargs,\n",
    "    # )\n",
    "\n",
    "\n",
    "async def create_rag_with_rerank():\n",
    "    \"\"\"Create LightRAG instance with rerank configuration\"\"\"\n",
    "\n",
    "    # Get embedding dimension\n",
    "    test_embedding = await embedding_func([\"test\"])\n",
    "    embedding_dim = test_embedding.shape[1]\n",
    "    print(f\"Detected embedding dimension: {embedding_dim}\")\n",
    "\n",
    "    # Method 1: Using custom rerank function\n",
    "    rag = LightRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        llm_model_func=llm_model_func,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_token_size=8192,\n",
    "            func=embedding_func,\n",
    "        ),\n",
    "        # Rerank Configuration - provide the rerank function\n",
    "        rerank_model_func=my_rerank_func,\n",
    "    )\n",
    "\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "\n",
    "    return rag\n",
    "\n",
    "\n",
    "async def create_rag_with_rerank_model():\n",
    "    \"\"\"Alternative: Create LightRAG instance using RerankModel wrapper\"\"\"\n",
    "\n",
    "    # Get embedding dimension\n",
    "    test_embedding = await embedding_func([\"test\"])\n",
    "    embedding_dim = test_embedding.shape[1]\n",
    "    print(f\"Detected embedding dimension: {embedding_dim}\")\n",
    "\n",
    "    # Method 2: Using RerankModel wrapper\n",
    "    rerank_model = RerankModel(\n",
    "        rerank_func=custom_rerank,\n",
    "        kwargs={\n",
    "            \"model\": \"xitao/bge-reranker-v2-m3:latest\",\n",
    "            \"base_url\": OLLAMA_BASE_URL,\n",
    "        },\n",
    "        # kwargs={\n",
    "        #     \"model\": \"BAAI/bge-reranker-v2-m3\",\n",
    "        #     \"base_url\": \"https://api.your-rerank-provider.com/v1/rerank\",\n",
    "        #     \"api_key\": \"your_rerank_api_key_here\",\n",
    "        # },\n",
    "    )\n",
    "\n",
    "    rag = LightRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        llm_model_func=llm_model_func,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_token_size=8192,\n",
    "            func=embedding_func,\n",
    "        ),\n",
    "        rerank_model_func=rerank_model.rerank,\n",
    "    )\n",
    "\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "\n",
    "    return rag\n",
    "\n",
    "\n",
    "async def test_rerank_with_different_settings():\n",
    "    \"\"\"\n",
    "    Test rerank functionality with different enable_rerank settings\n",
    "    \"\"\"\n",
    "    print(\"?? Setting up LightRAG with Rerank functionality...\")\n",
    "\n",
    "    rag = await create_rag_with_rerank()\n",
    "\n",
    "    # Insert sample documents\n",
    "    sample_docs = [\n",
    "        \"Reranking improves retrieval quality by re-ordering documents based on relevance.\",\n",
    "        \"LightRAG is a powerful retrieval-augmented generation system with multiple query modes.\",\n",
    "        \"Vector databases enable efficient similarity search in high-dimensional embedding spaces.\",\n",
    "        \"Natural language processing has evolved with large language models and transformers.\",\n",
    "        \"Machine learning algorithms can learn patterns from data without explicit programming.\",\n",
    "    ]\n",
    "\n",
    "    print(\"?? Inserting sample documents...\")\n",
    "    await rag.ainsert(sample_docs)\n",
    "\n",
    "    query = \"How does reranking improve retrieval quality?\"\n",
    "    print(f\"\\n?? Testing query: '{query}'\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Test with rerank enabled (default)\n",
    "    print(\"\\n?? Testing with enable_rerank=True (default):\")\n",
    "    result_with_rerank = await rag.aquery(\n",
    "        query,\n",
    "        param=QueryParam(\n",
    "            mode=\"naive\",\n",
    "            top_k=10,\n",
    "            chunk_top_k=5,\n",
    "            enable_rerank=True,  # Explicitly enable rerank\n",
    "        ),\n",
    "    )\n",
    "    print(f\"   Result length: {len(result_with_rerank)} characters\")\n",
    "    print(f\"   Preview: {result_with_rerank[:100]}...\")\n",
    "\n",
    "    # Test with rerank disabled\n",
    "    print(\"\\n?? Testing with enable_rerank=False:\")\n",
    "    result_without_rerank = await rag.aquery(\n",
    "        query,\n",
    "        param=QueryParam(\n",
    "            mode=\"naive\",\n",
    "            top_k=10,\n",
    "            chunk_top_k=5,\n",
    "            enable_rerank=False,  # Disable rerank\n",
    "        ),\n",
    "    )\n",
    "    print(f\"   Result length: {len(result_without_rerank)} characters\")\n",
    "    print(f\"   Preview: {result_without_rerank[:100]}...\")\n",
    "\n",
    "    # Test with default settings (enable_rerank defaults to True)\n",
    "    print(\"\\n?? Testing with default settings (enable_rerank defaults to True):\")\n",
    "    result_default = await rag.aquery(\n",
    "        query, param=QueryParam(mode=\"naive\", top_k=10, chunk_top_k=5)\n",
    "    )\n",
    "    print(f\"   Result length: {len(result_default)} characters\")\n",
    "    print(f\"   Preview: {result_default[:100]}...\")\n",
    "\n",
    "\n",
    "async def test_direct_rerank():\n",
    "    \"\"\"Test rerank function directly\"\"\"\n",
    "    print(\"\\n?? Direct Rerank API Test\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    documents = [\n",
    "        {\"content\": \"Reranking significantly improves retrieval quality\"},\n",
    "        {\"content\": \"LightRAG supports advanced reranking capabilities\"},\n",
    "        {\"content\": \"Vector search finds semantically similar documents\"},\n",
    "        {\"content\": \"Natural language processing with modern transformers\"},\n",
    "        {\"content\": \"The quick brown fox jumps over the lazy dog\"},\n",
    "    ]\n",
    "\n",
    "    query = \"rerank improve quality\"\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Documents: {len(documents)}\")\n",
    "\n",
    "    try:\n",
    "        reranked_docs = await custom_rerank(\n",
    "            query=query,\n",
    "            documents=documents,\n",
    "            model=\"BAAI/bge-reranker-v2-m3\",\n",
    "            base_url=\"https://api.your-rerank-provider.com/v1/rerank\",\n",
    "            api_key=\"your_rerank_api_key_here\",\n",
    "            top_n=3,\n",
    "        )\n",
    "\n",
    "        print(\"\\n? Rerank Results:\")\n",
    "        for i, doc in enumerate(reranked_docs):\n",
    "            score = doc.get(\"rerank_score\", \"N/A\")\n",
    "            content = doc.get(\"content\", \"\")[:60]\n",
    "            print(f\"  {i+1}. Score: {score:.4f} | {content}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"? Rerank failed: {e}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main example function\"\"\"\n",
    "    print(\"?? LightRAG Rerank Integration Example\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # Test rerank with different enable_rerank settings\n",
    "        await test_rerank_with_different_settings()\n",
    "\n",
    "        # Test direct rerank\n",
    "        await test_direct_rerank()\n",
    "\n",
    "        print(\"\\n? Example completed successfully!\")\n",
    "        print(\"\\n?? Key Points:\")\n",
    "        print(\"   ? Rerank is now controlled per query via 'enable_rerank' parameter\")\n",
    "        print(\"   ? Default value for enable_rerank is True\")\n",
    "        print(\"   ? Rerank function is configured at LightRAG initialization\")\n",
    "        print(\"   ? Per-query enable_rerank setting overrides default behavior\")\n",
    "        print(\n",
    "            \"   ? If enable_rerank=True but no rerank model is configured, a warning is issued\"\n",
    "        )\n",
    "        print(\"   ? Monitor API usage and costs when using rerank services\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n? Example failed: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "'''\n",
    "import asyncio\n",
    "import os\n",
    "import inspect\n",
    "import logging\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.ollama import ollama_model_complete, ollama_embed\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "\n",
    "WORKING_DIR = \"/workspace/yuchen/light_test\"\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=ollama_model_complete,\n",
    "    llm_model_name=\"qwen2.5:0.5b\",\n",
    "    llm_model_max_async=4,\n",
    "    llm_model_max_token_size=32768,\n",
    "    llm_model_kwargs={\"host\": \"http://dandelion-ollama-1:11434\", \"options\": {\"num_ctx\": 32768}},\n",
    "\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=768,\n",
    "        max_token_size=8192,\n",
    "        func=lambda texts: ollama_embed(\n",
    "            texts, embed_model=\"nomic-embed-text:latest\", host=\"http://dandelion-ollama-1:11434\"\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "with open(\"../data/PDF_file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    rag.insert(f.read())\n",
    "# async def main():\n",
    "#     with open(\"./data/PDF_file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#         await rag.ainsert(f.read())\n",
    "        \n",
    "# await main()\n",
    "\n",
    "# Perform naive search\n",
    "print(\n",
    "    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"naive\"))\n",
    ")\n",
    "\n",
    "# Perform local search\n",
    "print(\n",
    "    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"local\"))\n",
    ")\n",
    "\n",
    "# Perform global search\n",
    "print(\n",
    "    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"global\"))\n",
    ")\n",
    "\n",
    "# Perform hybrid search\n",
    "print(\n",
    "    rag.query(\"What are the top themes in this story?\", param=QueryParam(mode=\"hybrid\"))\n",
    ")\n",
    "\n",
    "# stream response\n",
    "resp = rag.query(\n",
    "    \"What are the top themes in this story?\",\n",
    "    param=QueryParam(mode=\"hybrid\", stream=True),\n",
    ")\n",
    "\n",
    "async def print_stream(stream):\n",
    "    async for chunk in stream:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "\n",
    "if inspect.isasyncgen(resp):\n",
    "    asyncio.run(print_stream(resp))\n",
    "\n",
    "else:\n",
    "    print(resp)\n",
    "\n",
    "    '''\n",
    "'''\n",
    "    INFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/workspace/yuchen/light_test/vdb_entities.json'} 0 data\n",
    "    INFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/workspace/yuchen/light_test/vdb_relationships.json'} 0 data\n",
    "    INFO:Init {'embedding_dim': 768, 'metric': 'cosine', 'storage_file': '/workspace/yuchen/light_test/vdb_chunks.json'} 0 data\n",
    "    Rerank is enabled but no rerank_model_func provided. Reranking will be skipped.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa7671-0bbd-40e3-a32a-f0479b171466",
   "metadata": {},
   "source": [
    "## 教學\n",
    "https://yourmi.csdn.net/67c529ffd649b06b61c6f690.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "491f9e05-99ee-493b-bf25-7f9c70374c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嘗試連接 LightRAG Server: http://lightrag:9621\n",
      "\n",
      "--- 插入文件 ---\n",
      "文件 1 插入成功: {'status': 'success', 'message': 'Text successfully received. Processing will continue in background.'}\n",
      "文件 2 插入成功: {'status': 'success', 'message': 'Text successfully received. Processing will continue in background.'}\n",
      "文件 3 插入成功: {'status': 'success', 'message': 'Text successfully received. Processing will continue in background.'}\n",
      "\n",
      "--- 執行查詢: 请解释AI、机器学习和深度学习之间的关系 ---\n",
      "查詢結果:\n",
      "{\n",
      "  \"response\": \"Sorry, I'm not able to provide an answer to that question.[no-context]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# LightRAG Server 在 Docker 網路中的服務名稱和埠號\n",
    "# 'lightrag' 是 docker-compose.yml 中定義的服務名稱\n",
    "LIGHTRAG_SERVER_URL = \"http://lightrag:9621\"\n",
    "\n",
    "print(f\"嘗試連接 LightRAG Server: {LIGHTRAG_SERVER_URL}\")\n",
    "\n",
    "# --- 步驟 1: 插入文件到 LightRAG Server ---\n",
    "documents = [\n",
    "    \"人工智能(AI)是计算机科学的一个分支,致力于开发能模拟人类智能的系统。\",\n",
    "    \"机器学习是AI的核心技术之一,它使计算机能够从数据中学习和改进。\",\n",
    "    \"深度学习是机器学习的一个子领域,使用多层神经网络处理复杂问题。\"\n",
    "]\n",
    "\n",
    "insert_url = f\"{LIGHTRAG_SERVER_URL}/documents/text\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "print(\"\\n--- 插入文件 ---\")\n",
    "for i, doc_content in enumerate(documents):\n",
    "    payload = {\n",
    "        \"text\": doc_content,\n",
    "        \"description\": f\"Document {i+1} about AI concepts\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(insert_url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status() # 如果請求不成功，會拋出 HTTPError\n",
    "        print(f\"文件 {i+1} 插入成功: {response.json()}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"文件 {i+1} 插入失敗: {e}\")\n",
    "        print(f\"回應內容: {response.text if 'response' in locals() else 'N/A'}\")\n",
    "        # 如果插入失敗，可能需要檢查 LightRAG Server 的日誌\n",
    "\n",
    "\n",
    "# --- 步驟 2: 觸發文件掃描 (如果文件是透過 volumes 映射到 input_dir) ---\n",
    "# 如果您是透過 /documents/text 插入，則不需要此步驟，因為內容已直接處理。\n",
    "# 但如果您的文件是放在 LightRAG 容器的 input_dir 中，則需要掃描。\n",
    "# 這裡假設您是透過 /documents/text 插入，所以此步驟是可選的。\n",
    "# scan_url = f\"{LIGHTRAG_SERVER_URL}/documents/scan\"\n",
    "# print(\"\\n--- 觸發文件掃描 (如果需要) ---\")\n",
    "# try:\n",
    "#     response = requests.post(scan_url, timeout=1800) # 設置較長的超時時間\n",
    "#     response.raise_for_status()\n",
    "#     print(f\"文件掃描觸發成功: {response.json()}\")\n",
    "# except requests.exceptions.RequestException as e:\n",
    "#     print(f\"文件掃描觸發失敗: {e}\")\n",
    "#     print(f\"回應內容: {response.text if 'response' in locals() else 'N/A'}\")\n",
    "\n",
    "# --- 步驟 3: 進行查詢測試 ---\n",
    "query_text = \"请解释AI、机器学习和深度学习之间的关系\"\n",
    "query_url = f\"{LIGHTRAG_SERVER_URL}/query\"\n",
    "\n",
    "query_payload = {\n",
    "    \"query\": query_text,\n",
    "    \"mode\": \"hybrid\" # LightRAG 支援的查詢模式，例如 \"hybrid\", \"local\", \"global\", \"mix\"\n",
    "}\n",
    "\n",
    "print(f\"\\n--- 執行查詢: {query_text} ---\")\n",
    "try:\n",
    "    response = requests.post(query_url, headers=headers, data=json.dumps(query_payload))\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    print(\"查詢結果:\")\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False)) # 格式化輸出並支援中文\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"查詢失敗: {e}\")\n",
    "    print(f\"回應內容: {response.text if 'response' in locals() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321f3509-0aba-4d23-b729-e6ef0fe6edf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ollama_chat_completion' from 'lightrag.llm' (/opt/conda/lib/python3.10/site-packages/lightrag/llm/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightrag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightRAG, QueryParam\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ollama_chat_completion, ollama_embedding\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingFunc\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 设置日志级别\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ollama_chat_completion' from 'lightrag.llm' (/opt/conda/lib/python3.10/site-packages/lightrag/llm/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import ollama_model_complete, ollama_embedding\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "\n",
    "# 设置日志级别\n",
    "logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.INFO)\n",
    "\n",
    "# 创建工作目录\n",
    "WORKING_DIR = \"./my_rag_project\"\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "# 初始化LightRAG,使用Ollama模型\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=ollama_model_complete,\n",
    "    llm_model_name=\"gemma2:2b\",  # 使用Gemma 2B模型\n",
    "    llm_model_max_async=4,  # 最大并发请求数\n",
    "    llm_model_max_token_size=32768,\n",
    "    llm_model_kwargs={\n",
    "        \"host\": \"http://localhost:11434\",  # Ollama服务地址\n",
    "        \"options\": {\"num_ctx\": 32768}  # 上下文窗口大小\n",
    "    },\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=768,\n",
    "        max_token_size=8192,\n",
    "        func=lambda texts: ollama_embedding(\n",
    "            texts, \n",
    "            embed_model=\"nomic-embed-text\",  # 使用nomic-embed-text作为嵌入模型\n",
    "            host=\"http://localhost:11434\"\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 插入文档并进行查询\n",
    "documents = [\n",
    "    \"人工智能(AI)是计算机科学的一个分支,致力于开发能模拟人类智能的系统。\",\n",
    "    \"机器学习是AI的核心技术之一,它使计算机能够从数中学习和改进。\",\n",
    "    \"深度学习是机器学习的一个子领域,使用多层神经网络处理复杂问题。\"\n",
    "]\n",
    "\n",
    "# 插入文档\n",
    "rag.insert(documents)\n",
    "\n",
    "# 使用不同的检索模式进行查询\n",
    "modes = [\"naive\", \"local\", \"global\", \"hybrid\"]\n",
    "query = \"请解释AI、机器学习和深度学习之间的关系\"\n",
    "\n",
    "for mode in modes:\n",
    "    print(f\"\\n使用{mode}模式的查询结果:\")\n",
    "    result = rag.query(query, param=QueryParam(mode=mode))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee3d6c9-83de-410c-b62f-8d81683be854",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightrag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightrag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightRAG, QueryParam\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightrag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ollama\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 建立 ChatOllama 實例\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from langchain_ollama import ChatOllama\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightrag'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm import ollama\n",
    "\n",
    "# 建立 ChatOllama 實例\n",
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "Chatllm = ollama(\n",
    "    base_url='http://dandelion-ollama-1:11434', \n",
    "    model=\"llama3.1:8b\",\n",
    "    temperature=0.0,\n",
    "    num_predict=51200\n",
    ")\n",
    "\n",
    "# 创建工作目录\n",
    "WORKING_DIR = \"./my_rag_project\"\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "# 初始化LightRAG\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=Chatllm\n",
    ")\n",
    "\n",
    "# 准备示例文档\n",
    "documents = [\n",
    "    \"人工智能(AI)是计算机科学的一个分支,致力于开发能模拟人类智能的系统。\",\n",
    "    \"机器学习是AI的核心技术之一,它使计算机能够从数据中学习和改进。\",\n",
    "    \"深度学习是机器学习的一个子领域,使用多层神经网络处理复杂问题。\"\n",
    "]\n",
    "\n",
    "# 插入文档\n",
    "rag.insert(documents)\n",
    "\n",
    "# 进行查询测试\n",
    "query = \"请解释AI、机器学习和深度学习之间的关系\"\n",
    "result = rag.query(query, param=QueryParam(mode=\"hybrid\"))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b95fc-e7bd-44a2-937f-c7b096f98d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d23da-8c46-4dea-8206-1a1de2399407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d435f2-adec-4e27-a57c-2380f6a6c01d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2e8c5-00a4-4cee-8a7d-d83d3c6f399a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7267f-00e7-420b-826c-a26a78b126a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55965b84-8242-4c51-bcd5-c92553846cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf035d6b-119a-4be6-a69d-0bbe4bf1e617",
   "metadata": {},
   "source": [
    "自訂 OllamaReranker 為 LangChain 的 DocumentCompressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795ce14-7d6a-4bd9-8c1d-1db01608069d",
   "metadata": {},
   "source": [
    "建立一個類別以包裝 Ollama api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d76abdcc-3fb1-4902-8261-093efd7f8f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T03:16:37.322976Z",
     "iopub.status.busy": "2025-07-28T03:16:37.321820Z",
     "iopub.status.idle": "2025-07-28T03:16:37.335746Z",
     "shell.execute_reply": "2025-07-28T03:16:37.334512Z",
     "shell.execute_reply.started": "2025-07-28T03:16:37.322937Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents.compressor import BaseDocumentCompressor\n",
    "# from langchain_core.documents.compressor import BaseDocumentCompressor\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Any\n",
    "import requests\n",
    "import re\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class OllamaReranker(BaseDocumentCompressor, BaseModel):\n",
    "    model: str = Field(default=\"xitao/bge-reranker-v2-m3\")\n",
    "    base_url: str = Field(default=\"http://localhost:11434\")\n",
    "\n",
    "    def compress_documents(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        query: str,\n",
    "        **kwargs: Any  # ©ú½T§i¶D Pydantic ¥i±µ¨ü¥ô¦óÃB¥~°Ñ¼Æ\n",
    "    ) -> List[Document]:\n",
    "        print(f\"[debug] compress_documents called with kwargs: {kwargs}\")\n",
    "\n",
    "        if not documents:\n",
    "            return []\n",
    "\n",
    "        prompt = f\"Query: {query}\\n\\n\"\n",
    "        for i, doc in enumerate(documents):\n",
    "            prompt += f\"[{i}] {doc.page_content.strip()}\\n\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        try:\n",
    "File /opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:259, in BaseRetriever.invoke(self, input, config, **kwargs)\n",
    "    257 kwargs_ = kwargs if self._expects_other_args else {}\n",
    "    258 if self._new_arg_supported:\n",
    "--> 259     result = self._get_relevant_documents(\n",
    "    260         input, run_manager=run_manager, **kwargs_\n",
    "    261     )\n",
    "    262 else:\n",
    "    263     result = self._get_relevant_documents(input, **kwargs_)\n",
    "\n",
    "File /opt/conda/lib/python3.10/site-packages/langchain/retrievers/contextual_compression.py:44, in ContextualCompressionRetriever._get_relevant_documents(self, query, run_manager, **kwargs)\n",
    "     40 docs = self.base_retriever.invoke(\n",
    "     41     query, config={\"callbacks\": run_manager.get_child()}, **kwargs\n",
    "     42 )\n",
    "     43 if docs:\n",
    "---> 44     compressed_docs = self.base_compressor.compress_documents(\n",
    "     45         docs, query, callbacks=run_manager.get_child()\n",
    "     46     )\n",
    "     47     return list(compressed_docs)\n",
    "     48 else:\n",
    "\n",
    "TypeError: OllamaReranker.compress_documents() got an unexpected keyword argument 'callbacks'\n",
    "\n",
    "\n",
    "DocumentCompressorMixin\n",
    "1\n",
    "11\n",
    "Python 3 (ipykernel) | Idle\n",
    "1\n",
    "Untitled.ipynb\n",
    "Ln 3, Col 16\n",
    "Mode: Command\n",
    "\n",
    "            response = requests.post(f\"{self.base_url}/api/generate\", json=payload)\n",
    "            result = response.json()\n",
    "            ranked_indices = self._parse_output_indices(result[\"response\"])\n",
    "        except Exception as e:\n",
    "            print(\"Ollama rerank failed:\", e)\n",
    "            return documents\n",
    "\n",
    "        return [documents[i] for i in ranked_indices if i < len(documents)]\n",
    "\n",
    "    def _parse_output_indices(self, text: str) -> List[int]:\n",
    "        return list(map(int, re.findall(r\"\\d+\", text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c7b68c-2e4e-4948-9624-efed25cba426",
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-28T03:25:39.562Z",
     "iopub.execute_input": "2025-07-28T03:25:31.992233Z",
     "iopub.status.busy": "2025-07-28T03:25:31.991859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349b54c743ba41c1b11b5467120744dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from FlagEmbedding import FlagLLMReranker\n",
    "reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "# reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n",
    "\n",
    "score = reranker.compute_score(['query', 'passage'])\n",
    "print(score)\n",
    "\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed7f6a-a2fd-4570-9e11-42239c723b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e57d258-3802-4d35-95b9-0aa4b8e750f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T02:58:49.305664Z",
     "iopub.status.busy": "2025-07-28T02:58:49.304965Z",
     "iopub.status.idle": "2025-07-28T02:58:50.675047Z",
     "shell.execute_reply": "2025-07-28T02:58:50.674596Z",
     "shell.execute_reply.started": "2025-07-28T02:58:49.305630Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 載入 PDF 文件\n",
    "loader = PyPDFLoader('./../data/PDF_file.pdf')\n",
    "# docs = loader.load()\n",
    "\n",
    "# 指定 text_splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "custom_chunks = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43259775-f7fe-4c7d-8fea-5b96a8963a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url='http://dandelion-ollama-1:11434', \n",
    "    model=\"nomic-embed-text\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5d049d4-f9cf-4f60-b841-2a53c833b2f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T02:58:54.913957Z",
     "iopub.status.busy": "2025-07-28T02:58:54.913516Z",
     "iopub.status.idle": "2025-07-28T02:58:57.663380Z",
     "shell.execute_reply": "2025-07-28T02:58:57.662151Z",
     "shell.execute_reply.started": "2025-07-28T02:58:54.913942Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# 將資料庫永久儲存在磁碟\n",
    "vectorstore_db = Chroma.from_documents(\n",
    "    custom_chunks, \n",
    "    embedding=embeddings, \n",
    "    persist_directory = \"./Chroma_db2\",\n",
    "    collection_name=\"little_prince_chroma\"\n",
    ")\n",
    "\n",
    "retriever_Chroma_db = vectorstore_db.as_retriever(search_kwargs={\"k\": 5}) # 檢索最相關的 3 個文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa515b49-4f55-4a29-88d8-2a79be3fb00f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T03:06:39.125668Z",
     "iopub.status.busy": "2025-07-28T03:06:39.124724Z",
     "iopub.status.idle": "2025-07-28T03:06:39.131106Z",
     "shell.execute_reply": "2025-07-28T03:06:39.129992Z",
     "shell.execute_reply.started": "2025-07-28T03:06:39.125633Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "ollama_reranker = OllamaReranker(\n",
    "    model=\"xitao/bge-reranker-v2-m3\",\n",
    "    base_url=\"http://dandelion-ollama-1:11434\"\n",
    ")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=ollama_reranker,\n",
    "    base_retriever=retriever_Chroma_db\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccdc3228-f218-4ff5-9ff6-18c3599f9b7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T03:16:44.835473Z",
     "iopub.status.busy": "2025-07-28T03:16:44.834124Z",
     "iopub.status.idle": "2025-07-28T03:16:44.958534Z",
     "shell.execute_reply": "2025-07-28T03:16:44.956946Z",
     "shell.execute_reply.started": "2025-07-28T03:16:44.835424Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OllamaReranker.compress_documents() got an unexpected keyword argument 'callbacks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mcompression_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m誰是小王子？\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(docs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;241m.\u001b[39mpage_content[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     emit_warning()\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:411\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_name:\n\u001b[1;32m    410\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:259\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 259\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/retrievers/contextual_compression.py:44\u001b[0m, in \u001b[0;36mContextualCompressionRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_retriever\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     41\u001b[0m     query, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_manager\u001b[38;5;241m.\u001b[39mget_child()}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[0;32m---> 44\u001b[0m     compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: OllamaReranker.compress_documents() got an unexpected keyword argument 'callbacks'"
     ]
    }
   ],
   "source": [
    "\n",
    "docs = compression_retriever.get_relevant_documents(\"誰是小王子？\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"[{i}] {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98293979-4089-42c7-950c-3317b0fe1881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbf8cf37-ccd5-4e10-87f4-a0244ef76480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T03:19:58.745104Z",
     "iopub.status.busy": "2025-07-28T03:19:58.744375Z",
     "iopub.status.idle": "2025-07-28T03:19:58.757701Z",
     "shell.execute_reply": "2025-07-28T03:19:58.756638Z",
     "shell.execute_reply.started": "2025-07-28T03:19:58.745071Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import Runnable\n",
    "from typing import List, Any\n",
    "import requests\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class ManualDocumentCompressor(Runnable, ABC):\n",
    "    @abstractmethod\n",
    "    def compress_documents(\n",
    "        self, documents: List[Document], query: str, **kwargs\n",
    "    ) -> List[Document]:\n",
    "        ...\n",
    "\n",
    "    def invoke(self, input: Any, **kwargs: Any) -> Any:\n",
    "        return self.compress_documents(input[\"documents\"], input[\"query\"], **kwargs)\n",
    "class OllamaReranker(ManualDocumentCompressor):\n",
    "    def __init__(self, model=\"xitao/bge-reranker-v2-m3\", base_url=\"http://localhost:11434\"):\n",
    "        self.model = model\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def compress_documents(self, documents: List[Document], query: str, **kwargs) -> List[Document]:\n",
    "        print(f\"[debug] compress_documents called with kwargs: {kwargs}\")\n",
    "\n",
    "        if not documents:\n",
    "            return []\n",
    "\n",
    "        prompt = f\"Query: {query}\\n\\n\"\n",
    "        for i, doc in enumerate(documents):\n",
    "            prompt += f\"[{i}] {doc.page_content.strip()}\\n\"\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(f\"{self.base_url}/api/generate\", json=payload)\n",
    "            result = response.json()\n",
    "            ranked_indices = self._parse_output_indices(result[\"response\"])\n",
    "        except Exception as e:\n",
    "            print(\"Ollama rerank failed:\", e)\n",
    "            return documents\n",
    "\n",
    "        return [documents[i] for i in ranked_indices if i < len(documents)]\n",
    "\n",
    "    def _parse_output_indices(self, text: str) -> List[int]:\n",
    "        return list(map(int, re.findall(r\"\\d+\", text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f1184e6-cf6c-4dbf-86a5-6a9bb956c1bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T03:20:17.440265Z",
     "iopub.status.busy": "2025-07-28T03:20:17.439636Z",
     "iopub.status.idle": "2025-07-28T03:20:17.443797Z",
     "shell.execute_reply": "2025-07-28T03:20:17.443111Z",
     "shell.execute_reply.started": "2025-07-28T03:20:17.440239Z"
    }
   },
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=ollama_reranker,  # ¡ö ³o¤´µM OK¡I\n",
    "    base_retriever=retriever_Chroma_db\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9703b78-f648-4327-943a-b9be8d9c6efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T03:22:11.908841Z",
     "iopub.status.busy": "2025-07-28T03:22:11.908378Z",
     "iopub.status.idle": "2025-07-28T03:22:12.013605Z",
     "shell.execute_reply": "2025-07-28T03:22:12.012391Z",
     "shell.execute_reply.started": "2025-07-28T03:22:11.908809Z"
    }
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ContextualCompressionRetriever\nbase_compressor\n  Input should be a valid dictionary or instance of BaseDocumentCompressor [type=model_type, input_value=<__main__.OllamaReranker object at 0x7b9f096d9570>, input_type=OllamaReranker]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextualCompressionRetriever\n\u001b[1;32m      3\u001b[0m ollama_reranker \u001b[38;5;241m=\u001b[39m OllamaReranker(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxitao/bge-reranker-v2-m3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://dandelion-ollama-1:11434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m compression_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mContextualCompressionRetriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mollama_reranker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_retriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever_Chroma_db\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/load/serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic/main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ContextualCompressionRetriever\nbase_compressor\n  Input should be a valid dictionary or instance of BaseDocumentCompressor [type=model_type, input_value=<__main__.OllamaReranker object at 0x7b9f096d9570>, input_type=OllamaReranker]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "ollama_reranker = OllamaReranker(\n",
    "    model=\"xitao/bge-reranker-v2-m3\",\n",
    "    base_url=\"http://dandelion-ollama-1:11434\"\n",
    ")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=ollama_reranker,\n",
    "    base_retriever=retriever_Chroma_db\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6087fe97-2b44-44a3-9932-cb4013290053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T03:20:30.991677Z",
     "iopub.status.busy": "2025-07-28T03:20:30.991010Z",
     "iopub.status.idle": "2025-07-28T03:20:31.088015Z",
     "shell.execute_reply": "2025-07-28T03:20:31.086929Z",
     "shell.execute_reply.started": "2025-07-28T03:20:30.991629Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OllamaReranker.compress_documents() got an unexpected keyword argument 'callbacks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mcompression_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m誰是小王子？\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     emit_warning()\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:411\u001b[0m, in \u001b[0;36mBaseRetriever.get_relevant_documents\u001b[0;34m(self, query, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_name:\n\u001b[1;32m    410\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_name\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_core/retrievers.py:259\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m kwargs_ \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[0;32m--> 259\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/retrievers/contextual_compression.py:44\u001b[0m, in \u001b[0;36mContextualCompressionRetriever._get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_retriever\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     41\u001b[0m     query, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_manager\u001b[38;5;241m.\u001b[39mget_child()}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs:\n\u001b[0;32m---> 44\u001b[0m     compressed_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_compressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(compressed_docs)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: OllamaReranker.compress_documents() got an unexpected keyword argument 'callbacks'"
     ]
    }
   ],
   "source": [
    "docs = compression_retriever.get_relevant_documents(\"誰是小王子？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bc49b-3139-4d73-8809-d3aad291d75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
