<!doctype html><html lang=zh-Hant-TW dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="簡單說明 FlagEmbedding 與 Hugging Face Transformers 實作 BGE Reranker 模型的差異與用途。"><title>BAAI/bge-reranker-v2-m3 — Hugging Face 官方整理</title><link rel=canonical href=https://Dandelionlibra.github.io/post/note/bge-reranker-hf-flagembedding-note/><link rel=stylesheet href=/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css><meta property='og:title' content="BAAI/bge-reranker-v2-m3 — Hugging Face 官方整理"><meta property='og:description' content="簡單說明 FlagEmbedding 與 Hugging Face Transformers 實作 BGE Reranker 模型的差異與用途。"><meta property='og:url' content='https://Dandelionlibra.github.io/post/note/bge-reranker-hf-flagembedding-note/'><meta property='og:site_name' content='YuChen'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='AI'><meta property='article:tag' content='Large language model'><meta property='article:tag' content='LLM'><meta property='article:tag' content='Reranker'><meta property='article:tag' content='FlagEmbedding'><meta property='article:tag' content='Hugging Face'><meta property='article:tag' content='BGE'><meta property='article:published_time' content='2025-07-23T03:51:00+08:00'><meta property='article:modified_time' content='2025-07-23T03:51:00+08:00'><meta name=twitter:title content="BAAI/bge-reranker-v2-m3 — Hugging Face 官方整理"><meta name=twitter:description content="簡單說明 FlagEmbedding 與 Hugging Face Transformers 實作 BGE Reranker 模型的差異與用途。"></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column compact"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_a0443a4a03d93adb.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>YuChen</a></h1><h2 class=site-description></h2></div></header><ol class=menu-social><li><a href=https://github.com/Dandelionlibra target=_blank title=Github rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://x.com/ target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/about><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>About</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/note/>Note</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/post/note/bge-reranker-hf-flagembedding-note/>BAAI/bge-reranker-v2-m3 — Hugging Face 官方整理</a></h2><h3 class=article-subtitle>簡單說明 FlagEmbedding 與 Hugging Face Transformers 實作 BGE Reranker 模型的差異與用途。</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 23, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>4 minute read</time></div></footer></div></header><section class=article-content><h2 id=reranker-模型與函式庫使用差異筆記>Reranker 模型與函式庫使用差異筆記</h2><p>這份筆記整理了 FlagEmbedding 和 Hugging Face Transformers 在實作不同 Reranker 模型（標準型、LLM 型、分層式 LLM 型）時的關鍵差異。</p><hr><h3 id=核心實作比較>核心實作比較</h3><h4 id=flagembedding>FlagEmbedding</h4><p><code>FlagEmbedding</code> 函式庫提供了更簡潔、高度封裝的 API，適合快速整合和高效能應用。</p><ol><li>標準 Reranker (bge-reranker-base / large / v2-m3)</li></ol><ul><li>方法: 使用 <code>FlagReranker</code> 類別。</li><li>特點: 最直接、優化的方法，簡化模型載入和計算。</li><li>程式碼範例:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> FlagEmbedding <span style=color:#f92672>import</span> FlagReranker
</span></span><span style=display:flex><span><span style=color:#75715e># Setting use_fp16 to True speeds up computation with a slight performance degradation</span>
</span></span><span style=display:flex><span>reranker <span style=color:#f92672>=</span> FlagReranker(<span style=color:#e6db74>&#39;BAAI/bge-reranker-v2-m3&#39;</span>, use_fp16<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>score <span style=color:#f92672>=</span> reranker<span style=color:#f92672>.</span>compute_score([<span style=color:#e6db74>&#39;query&#39;</span>, <span style=color:#e6db74>&#39;passage&#39;</span>])
</span></span><span style=display:flex><span>print(score) <span style=color:#75715e># -5.65234375</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Map the scores into 0-1 by set &#34;normalize=True&#34;, which will apply sigmoid function to the score</span>
</span></span><span style=display:flex><span>score <span style=color:#f92672>=</span> reranker<span style=color:#f92672>.</span>compute_score([<span style=color:#e6db74>&#39;query&#39;</span>, <span style=color:#e6db74>&#39;passage&#39;</span>], normalize<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>print(score) <span style=color:#75715e># 0.003497010252573502</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>scores <span style=color:#f92672>=</span> reranker<span style=color:#f92672>.</span>compute_score([[<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;hi&#39;</span>], [<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.&#39;</span>]])
</span></span><span style=display:flex><span>print(scores) <span style=color:#75715e># [-8.1875, 5.26171875]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># set &#34;normalize=True&#34;</span>
</span></span><span style=display:flex><span>scores <span style=color:#f92672>=</span> reranker<span style=color:#f92672>.</span>compute_score([[<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;hi&#39;</span>], [<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.&#39;</span>]], normalize<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>print(scores) <span style=color:#75715e># [0.00027803096387751553, 0.9948403768236574]</span>
</span></span></code></pre></div></li></ul><ol start=2><li>LLM-based Reranker</li></ol><ul><li>方法: 利用 <code>FlagLLMReranker</code> 類別。</li><li>特點: 將大型語言模型（如 Llama）作為 Reranker，利用其語言理解能力進行細緻排序。需要大量 VRAM (>40G)。</li><li>程式碼範例:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> FlagEmbedding <span style=color:#f92672>import</span> FlagLLMReranker
</span></span><span style=display:flex><span><span style=color:#75715e># Setting use_fp16 to True speeds up computation with a slight performance degradation</span>
</span></span><span style=display:flex><span>reranker <span style=color:#f92672>=</span> FlagLLMReranker(<span style=color:#e6db74>&#39;BAAI/bge-reranker-v2-gemma&#39;</span>, use_fp16<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>score <span style=color:#f92672>=</span> reranker<span style=color:#f92672>.</span>compute_score([<span style=color:#e6db74>&#39;query&#39;</span>, <span style=color:#e6db74>&#39;passage&#39;</span>])
</span></span><span style=display:flex><span>print(score)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>scores <span style=color:#f92672>=</span> reranker<span style=color:#f92672>.</span>compute_score([[<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;hi&#39;</span>], [<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.&#39;</span>]])
</span></span><span style=display:flex><span>print(scores)
</span></span></code></pre></div></li></ul><ol start=3><li>LLM-based Layerwise Reranker</li></ol><ul><li>方法: 透過 FlagLLMReranker 的 compute_score_layerwise 方法。</li><li>特點: 可從 LLM 的不同層獲取分數，提供對模型決策過程的深入洞察。</li><li>程式碼範例:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> FlagEmbedding <span style=color:#f92672>import</span> LayerWiseFlagLLMReranker
</span></span><span style=display:flex><span><span style=color:#75715e># Setting use_fp16 to True speeds up computation with a slight performance degradation</span>
</span></span><span style=display:flex><span>reranker <span style=color:#f92672>=</span> LayerWiseFlagLLMReranker(<span style=color:#e6db74>&#39;BAAI/bge-reranker-v2-minicpm-layerwise&#39;</span>, use_fp16<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Adjusting &#39;cutoff_layers&#39; to pick which layers are used for computing the score.</span>
</span></span><span style=display:flex><span>score <span style=color:#f92672>=</span> reranker<span style=color:#f92672>.</span>compute_score([<span style=color:#e6db74>&#39;query&#39;</span>, <span style=color:#e6db74>&#39;passage&#39;</span>], cutoff_layers<span style=color:#f92672>=</span>[<span style=color:#ae81ff>28</span>]) 
</span></span><span style=display:flex><span>print(score)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>scores <span style=color:#f92672>=</span> reranker<span style=color:#f92672>.</span>compute_score([[<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;hi&#39;</span>], [<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.&#39;</span>]], cutoff_layers<span style=color:#f92672>=</span>[<span style=color:#ae81ff>28</span>])
</span></span><span style=display:flex><span>print(scores)
</span></span></code></pre></div></li></ul><hr><h4 id=hugging-face-transformers>Hugging Face Transformers</h4><p>Hugging Face Transformers 函式庫提供了更通用和靈活的方法，適合需要深度自訂和學術研究的場景。</p><ol><li>標準 Reranker (bge-reranker-base / large / v2-m3)</li></ol><ul><li>方法: 載入 AutoTokenizer 和 AutoModelForSequenceClassification。</li><li>特點: 標準流程，提供更多自訂空間。</li><li>程式碼範例:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForSequenceClassification, AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;BAAI/bge-reranker-v2-m3&#39;</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;BAAI/bge-reranker-v2-m3&#39;</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pairs <span style=color:#f92672>=</span> [[<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;hi&#39;</span>], [<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.&#39;</span>]]
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad(): <span style=color:#75715e># 無梯度下降</span>
</span></span><span style=display:flex><span>   inputs <span style=color:#f92672>=</span> tokenizer(pairs, padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>)
</span></span><span style=display:flex><span>   scores <span style=color:#f92672>=</span> model(<span style=color:#f92672>**</span>inputs, return_dict<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>logits<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, )<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>   print(scores)
</span></span></code></pre></div></li></ul><ol start=2><li>LLM-based Reranker</li></ol><ul><li>方法: 載入 AutoTokenizer 和 AutoModelForCausalLM。</li><li>特點: 需要手動處理模型輸出以獲得分數，提供最大的靈活性和控制力。</li><li>程式碼範例:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_inputs</span>(pairs, tokenizer, prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>):
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>if</span> prompt <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>      prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either &#39;Yes&#39; or &#39;No&#39;.&#34;</span>
</span></span><span style=display:flex><span>   sep <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>   prompt_inputs <span style=color:#f92672>=</span> tokenizer(prompt,
</span></span><span style=display:flex><span>                              return_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                              add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)[<span style=color:#e6db74>&#39;input_ids&#39;</span>]
</span></span><span style=display:flex><span>   sep_inputs <span style=color:#f92672>=</span> tokenizer(sep,
</span></span><span style=display:flex><span>                           return_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                           add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)[<span style=color:#e6db74>&#39;input_ids&#39;</span>]
</span></span><span style=display:flex><span>   inputs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>for</span> query, passage <span style=color:#f92672>in</span> pairs:
</span></span><span style=display:flex><span>      query_inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;A: </span><span style=color:#e6db74>{</span>query<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>,
</span></span><span style=display:flex><span>                                 return_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                                 add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>                                 max_length<span style=color:#f92672>=</span>max_length <span style=color:#f92672>*</span> <span style=color:#ae81ff>3</span> <span style=color:#f92672>//</span> <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>                                 truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>      passage_inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;B: </span><span style=color:#e6db74>{</span>passage<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>,
</span></span><span style=display:flex><span>                                 return_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                                 add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>                                 max_length<span style=color:#f92672>=</span>max_length,
</span></span><span style=display:flex><span>                                 truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>      item <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>prepare_for_model(
</span></span><span style=display:flex><span>            [tokenizer<span style=color:#f92672>.</span>bos_token_id] <span style=color:#f92672>+</span> query_inputs[<span style=color:#e6db74>&#39;input_ids&#39;</span>],
</span></span><span style=display:flex><span>            sep_inputs <span style=color:#f92672>+</span> passage_inputs[<span style=color:#e6db74>&#39;input_ids&#39;</span>],
</span></span><span style=display:flex><span>            truncation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;only_second&#39;</span>,
</span></span><span style=display:flex><span>            max_length<span style=color:#f92672>=</span>max_length,
</span></span><span style=display:flex><span>            padding<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            return_attention_mask<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            return_token_type_ids<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>      item[<span style=color:#e6db74>&#39;input_ids&#39;</span>] <span style=color:#f92672>=</span> item[<span style=color:#e6db74>&#39;input_ids&#39;</span>] <span style=color:#f92672>+</span> sep_inputs <span style=color:#f92672>+</span> prompt_inputs
</span></span><span style=display:flex><span>      item[<span style=color:#e6db74>&#39;attention_mask&#39;</span>] <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> len(item[<span style=color:#e6db74>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>      inputs<span style=color:#f92672>.</span>append(item)
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>return</span> tokenizer<span style=color:#f92672>.</span>pad(
</span></span><span style=display:flex><span>            inputs,
</span></span><span style=display:flex><span>            padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            max_length<span style=color:#f92672>=</span>max_length <span style=color:#f92672>+</span> len(sep_inputs) <span style=color:#f92672>+</span> len(prompt_inputs),
</span></span><span style=display:flex><span>            pad_to_multiple_of<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>            return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>,
</span></span><span style=display:flex><span>   )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;BAAI/bge-reranker-v2-gemma&#39;</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;BAAI/bge-reranker-v2-gemma&#39;</span>)
</span></span><span style=display:flex><span>yes_loc <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>&#39;Yes&#39;</span>, add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)[<span style=color:#e6db74>&#39;input_ids&#39;</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pairs <span style=color:#f92672>=</span> [[<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;hi&#39;</span>], [<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.&#39;</span>]]
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>   inputs <span style=color:#f92672>=</span> get_inputs(pairs, tokenizer)
</span></span><span style=display:flex><span>   scores <span style=color:#f92672>=</span> model(<span style=color:#f92672>**</span>inputs, return_dict<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>logits[:, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, yes_loc]<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, )<span style=color:#f92672>.</span>float()
</span></span><span style=display:flex><span>   print(scores)
</span></span></code></pre></div></li></ul><ol start=3><li>LLM-based Layerwise Reranker</li></ol><ul><li>方法: 透過手動存取 AutoModelForCausalLM 的隱藏層輸出或注意力權重，並自行計算分數。</li><li>特點: 提供對 LLM 內部決策過程最細緻的控制和分析，但實作複雜度高，需要深入理解模型架構。</li><li>程式碼範例:<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_inputs</span>(pairs, tokenizer, prompt<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>):
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>if</span> prompt <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>      prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either &#39;Yes&#39; or &#39;No&#39;.&#34;</span>
</span></span><span style=display:flex><span>   sep <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>   prompt_inputs <span style=color:#f92672>=</span> tokenizer(prompt,
</span></span><span style=display:flex><span>                              return_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                              add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)[<span style=color:#e6db74>&#39;input_ids&#39;</span>]
</span></span><span style=display:flex><span>   sep_inputs <span style=color:#f92672>=</span> tokenizer(sep,
</span></span><span style=display:flex><span>                           return_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                           add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)[<span style=color:#e6db74>&#39;input_ids&#39;</span>]
</span></span><span style=display:flex><span>   inputs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>for</span> query, passage <span style=color:#f92672>in</span> pairs:
</span></span><span style=display:flex><span>      query_inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;A: </span><span style=color:#e6db74>{</span>query<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>,
</span></span><span style=display:flex><span>                                 return_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                                 add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>                                 max_length<span style=color:#f92672>=</span>max_length <span style=color:#f92672>*</span> <span style=color:#ae81ff>3</span> <span style=color:#f92672>//</span> <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>                                 truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>      passage_inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;B: </span><span style=color:#e6db74>{</span>passage<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>,
</span></span><span style=display:flex><span>                                 return_tensors<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>                                 add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>                                 max_length<span style=color:#f92672>=</span>max_length,
</span></span><span style=display:flex><span>                                 truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>      item <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>prepare_for_model(
</span></span><span style=display:flex><span>            [tokenizer<span style=color:#f92672>.</span>bos_token_id] <span style=color:#f92672>+</span> query_inputs[<span style=color:#e6db74>&#39;input_ids&#39;</span>],
</span></span><span style=display:flex><span>            sep_inputs <span style=color:#f92672>+</span> passage_inputs[<span style=color:#e6db74>&#39;input_ids&#39;</span>],
</span></span><span style=display:flex><span>            truncation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;only_second&#39;</span>,
</span></span><span style=display:flex><span>            max_length<span style=color:#f92672>=</span>max_length,
</span></span><span style=display:flex><span>            padding<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            return_attention_mask<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            return_token_type_ids<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>            add_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>      )
</span></span><span style=display:flex><span>      item[<span style=color:#e6db74>&#39;input_ids&#39;</span>] <span style=color:#f92672>=</span> item[<span style=color:#e6db74>&#39;input_ids&#39;</span>] <span style=color:#f92672>+</span> sep_inputs <span style=color:#f92672>+</span> prompt_inputs
</span></span><span style=display:flex><span>      item[<span style=color:#e6db74>&#39;attention_mask&#39;</span>] <span style=color:#f92672>=</span> [<span style=color:#ae81ff>1</span>] <span style=color:#f92672>*</span> len(item[<span style=color:#e6db74>&#39;input_ids&#39;</span>])
</span></span><span style=display:flex><span>      inputs<span style=color:#f92672>.</span>append(item)
</span></span><span style=display:flex><span>   <span style=color:#66d9ef>return</span> tokenizer<span style=color:#f92672>.</span>pad(
</span></span><span style=display:flex><span>            inputs,
</span></span><span style=display:flex><span>            padding<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            max_length<span style=color:#f92672>=</span>max_length <span style=color:#f92672>+</span> len(sep_inputs) <span style=color:#f92672>+</span> len(prompt_inputs),
</span></span><span style=display:flex><span>            pad_to_multiple_of<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>,
</span></span><span style=display:flex><span>            return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;pt&#39;</span>,
</span></span><span style=display:flex><span>   )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;BAAI/bge-reranker-v2-minicpm-layerwise&#39;</span>, trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;BAAI/bge-reranker-v2-minicpm-layerwise&#39;</span>, trust_remote_code<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, torch_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>bfloat16)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pairs <span style=color:#f92672>=</span> [[<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;hi&#39;</span>], [<span style=color:#e6db74>&#39;what is panda?&#39;</span>, <span style=color:#e6db74>&#39;The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.&#39;</span>]]
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>   inputs <span style=color:#f92672>=</span> get_inputs(pairs, tokenizer)<span style=color:#f92672>.</span>to(model<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>   all_scores <span style=color:#f92672>=</span> model(<span style=color:#f92672>**</span>inputs, return_dict<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, cutoff_layers<span style=color:#f92672>=</span>[<span style=color:#ae81ff>28</span>])
</span></span><span style=display:flex><span>   all_scores <span style=color:#f92672>=</span> [scores[:, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, )<span style=color:#f92672>.</span>float() <span style=color:#66d9ef>for</span> scores <span style=color:#f92672>in</span> all_scores[<span style=color:#ae81ff>0</span>]]
</span></span><span style=display:flex><span>   print(all_scores)
</span></span></code></pre></div></li></ul><hr><h3 id=核心概念解析>核心概念解析</h3><h4 id=標準-reranker-cross-encoder>標準 Reranker (Cross-Encoder)</h4><ul><li>原理: 將「查詢」和「文件」成對地同時輸入到模型中，模型利用兩者之間的交互資訊判斷相關性。</li><li>輸出: 單一相關性分數。</li><li>流程: 查詢 + 文件 → Cross-Encoder 模型 → 相關性分數</li></ul><h4 id=llm-based-reranker>LLM-based Reranker</h4><ul><li>原理: 使用完整的大型語言模型（LLM）作為 Reranker，利用其龐大知識和推理能力理解深層語義關係。</li><li>輸出: 通常透過特定 token（如 [Yes] 或 [No]）的機率計算分數。</li><li>流程: 查詢 + 文件 → 大型語言模型 (LLM) → 基於生成機率的分數</li></ul><h4 id=總結與比較>總結與比較</h4><div class=table-wrapper><table><thead><tr><th>比較維度</th><th>FlagEmbedding</th><th>Hugging Face Transformers</th></tr></thead><tbody><tr><td>易用性</td><td>高（API 封裝良好）</td><td>中（需要更多手動設定）</td></tr><tr><td>靈活性</td><td>中（專為 Reranking 優化）</td><td>高（可完全自訂流程）</td></tr><tr><td>特色功能</td><td>Layerwise 分數計算</td><td>與整個 Hugging Face 生態系無縫接軌</td></tr><tr><td>推薦使用情境</td><td>需要快速實現高效能 Reranking 的應用</td><td>需要深度自訂模型行為或進行學術研究</td></tr></tbody></table></div></section><footer class=article-footer><section class=article-tags><a href=/tags/ai/>AI</a>
<a href=/tags/large-language-model/>Large Language Model</a>
<a href=/tags/llm/>LLM</a>
<a href=/tags/reranker/>Reranker</a>
<a href=/tags/flagembedding/>FlagEmbedding</a>
<a href=/tags/hugging-face/>Hugging Face</a>
<a href=/tags/bge/>BGE</a></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/post/langchain/retrieverqa-2/><div class=article-details><h2 class=article-title>LangChain 記憶型檢索問答：《小王子》文本互動實踐</h2></div></a></article><article><a href=/post/langchain/retrieverqa-1/><div class=article-details><h2 class=article-title>使用 Langchain 框架進行檢索提問</h2></div></a></article><article><a href=/post/langchain/uselangchain-4/><div class=article-details><h2 class=article-title>LangChain 基本使用-4</h2></div></a></article><article><a href=/post/langchain/uselangchain-3/><div class=article-details><h2 class=article-title>LangChain 基本使用-3</h2></div></a></article><article><a href=/post/langchain/uselangchain-2/><div class=article-details><h2 class=article-title>LangChain 基本使用-2</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2025 YuChen</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>