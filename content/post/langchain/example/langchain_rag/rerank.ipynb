{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac0084f-e129-461a-b5a8-349b45e22584",
   "metadata": {},
   "source": [
    "all retrievers\n",
    "\n",
    "https://python.langchain.com/docs/integrations/retrievers/\n",
    "\n",
    "\n",
    "\n",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7471fe2-4b9c-49fb-b900-6383e8cf6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 bge-reranker\n",
    "!pip install -U FlagEmbedding\n",
    "\n",
    "# FlagEmbedding 是什麼？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19518de6-eea1-43b0-a4f8-8dec9e1ff065",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 在 jupyter 中顯示進度條功能\n",
    "# ! pip install ipywidgets\n",
    "# ! jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "# ! jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53023b3-d5e2-469b-81dc-26c15d04774a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534daea790bd46268f5731a83e2052b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "widgets.IntSlider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c94df3c-bccb-4362-9ae5-9595257b6eba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbextension enable --py widgetsnbextension --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04f37b2-65ef-4efa-96fa-3b6c471a3ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a5e58a-1083-444a-843e-8245da228247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.6501]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-v2-m3\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "query = \"query\"\n",
    "passage = \"passage\"\n",
    "\n",
    "inputs = tokenizer(query, passage, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**inputs).logits\n",
    "#     print(logits)  # ¨S crash ¤~·|Åã¥Ü\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0958a321-565b-4284-85ae-027d2fe494fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw score: -2.4507484436035156\n",
      "Normalized score (sigmoid): 0.07938383519649506\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F  # ¥Î©ó sigmoid\n",
    "\n",
    "\n",
    "# ¸ü¤J¼Ò«¬»P tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-v2-m3\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# ¬d¸ß»P¤å¥ó\n",
    "query = \"what is panda?\"\n",
    "passage = \"The giant panda is native to China.\"\n",
    "\n",
    "# ½s½X¨ÃÂà¨ì GPU\n",
    "inputs = tokenizer(query, passage, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "# ¤£°µ¤Ï¦V¶Ç¼½\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits  # shape: [1, 1]\n",
    "    score = logits[0].item()  # ­ì©l¤À¼Æ\n",
    "    normalized_score = torch.sigmoid(logits)[0].item()  # 0~1 ½d³ò\n",
    "\n",
    "# Åã¥Üµ²ªG\n",
    "print(f\"Raw score: {score}\")\n",
    "print(f\"Normalized score (sigmoid): {normalized_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f839df-ca43-4778-bdb9-0776963e9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "score = reranker.compute_score(['query', 'passage'])\n",
    "print(score) # -5.65234375\n",
    "\n",
    "# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\n",
    "score = reranker.compute_score(['query', 'passage'], normalize=True)\n",
    "print(score) # 0.003497010252573502\n",
    "\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\n",
    "print(scores) # [-8.1875, 5.26171875]\n",
    "\n",
    "# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\n",
    "print(scores) # [0.00027803096387751553, 0.9948403768236574]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277d5f3-c8c5-497a-8d56-a4abb7769c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e86fabc13b4e99a5b51b5f2e30be39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from FlagEmbedding import FlagLLMReranker\n",
    "reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "# reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n",
    "\n",
    "score = reranker.compute_score(['query', 'passage'])\n",
    "print(score)\n",
    "\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fda89-1817-4462-9308-8e7e9c84783b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type minicpm. This is not supported for all configurations of models and can yield errors.\n",
      "LayerWiseMiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of LayerWiseMiniCPMForCausalLM were not initialized from the model checkpoint at BAAI/bge-reranker-base and are newly initialized: ['lm_head.0.linear_head.weight', 'lm_head.1.linear_head.weight', 'lm_head.2.linear_head.weight', 'lm_head.3.linear_head.weight', 'lm_head.4.linear_head.weight', 'lm_head.weight', 'model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LayerWiseMiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "LayerWiseMiniCPMForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import LayerWiseFlagLLMReranker\n",
    "reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "# reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n",
    "\n",
    "score = reranker.compute_score(['query', 'passage'], cutoff_layers=[28]) # Adjusting 'cutoff_layers' to pick which layers are used for computing the score.\n",
    "print(score)\n",
    "\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], cutoff_layers=[28])\n",
    "print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151908f-16c8-4250-9e00-b943daca6944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-base', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "# reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n",
    "\n",
    "score = reranker.compute_score(['query', 'passage'], cutoff_layers=[28]) # Adjusting 'cutoff_layers' to pick which layers are used for computing the score.\n",
    "print(score)\n",
    "\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], cutoff_layers=[28])\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff8d394-f671-4af3-8d9f-287c979cffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = reranker.compute_score(['query', 'passage'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dbea41-f0bd-4b34-a10e-557cc78b0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-base', use_fp16=False, device=\"cuda\")\n",
    "score = reranker.compute_score(['query', 'passage'])\n",
    "print(next(reranker.model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8dbe5a-5e33-4c8d-b096-958bcf39bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel will die\n",
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "score = reranker.compute_score(['query', 'passage'])\n",
    "print(score) # -5.65234375\n",
    "\n",
    "# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\n",
    "score = reranker.compute_score(['query', 'passage'], normalize=True)\n",
    "print(score) # 0.003497010252573502\n",
    "\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\n",
    "print(scores) # [-8.1875, 5.26171875]\n",
    "\n",
    "# You can map the scores into 0-1 by set \"normalize=True\", which will apply sigmoid function to the score\n",
    "scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']], normalize=True)\n",
    "print(scores) # [0.00027803096387751553, 0.9948403768236574]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f1bb35a-7ab6-4961-9d5f-2295a67498d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.retrievers import ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3639382-c126-400a-b829-ceda67e1feb9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m compressor \u001b[38;5;241m=\u001b[39m CrossEncoderReranker(model\u001b[38;5;241m=\u001b[39mmodel, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# °²³]§A¤w¦³ base_retriever¡]¨Ò¦p FAISS retriever¡^\u001b[39;00m\n\u001b[1;32m      8\u001b[0m compression_retriever \u001b[38;5;241m=\u001b[39m ContextualCompressionRetriever(\n\u001b[0;32m----> 9\u001b[0m     base_retriever\u001b[38;5;241m=\u001b[39m\u001b[43mbase_retriever\u001b[49m,\n\u001b[1;32m     10\u001b[0m     base_compressor\u001b[38;5;241m=\u001b[39mcompressor\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ¬d¸ß rerank µ²ªG\u001b[39;00m\n\u001b[1;32m     14\u001b[0m docs \u001b[38;5;241m=\u001b[39m compression_retriever\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m§Aªº¬d¸ß¤º®e\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_retriever' is not defined"
     ]
    }
   ],
   "source": [
    "# ªì©l¤Æ CrossEncoder¡]¹w³]¨Ï¥Î BGE reranker¡^\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "\n",
    "# ¥]¸Ë CrossEncoderReranker\n",
    "compressor = CrossEncoderReranker(model=model, top_n=5)\n",
    "\n",
    "# °²³]§A¤w¦³ base_retriever¡]¨Ò¦p FAISS retriever¡^\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    base_compressor=compressor\n",
    ")\n",
    "\n",
    "# ¬d¸ß rerank µ²ªG\n",
    "docs = compression_retriever.invoke(\"查詢內容：\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e12d0-c35e-4816-993c-450c3b093c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d87d7-c257-4778-85f5-e8dec035356b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f99b92f-5668-4ad8-bcf7-0c0cad12b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever # retriver\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('../data/PDF_file.pdf')\n",
    "docs = loader.load_and_split()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ddf537e-5d77-4e1c-b359-4f4b2ecf48eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import rank_bm25, please install with `pip install rank_bm25`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/retrievers/bm25.py:55\u001b[0m, in \u001b[0;36mBM25Retriever.from_texts\u001b[0;34m(cls, texts, metadatas, ids, bm25_params, preprocess_func, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rank_bm25'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bm25_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mBM25Retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m bm25_retriever\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/retrievers/bm25.py:102\u001b[0m, in \u001b[0;36mBM25Retriever.from_documents\u001b[0;34m(cls, documents, bm25_params, preprocess_func, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03mCreate a BM25Retriever from a list of Documents.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    A BM25Retriever instance.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m texts, metadatas, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;241m*\u001b[39m((d\u001b[38;5;241m.\u001b[39mpage_content, d\u001b[38;5;241m.\u001b[39mmetadata, d\u001b[38;5;241m.\u001b[39mid) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents)\n\u001b[1;32m    101\u001b[0m )\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbm25_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbm25_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/retrievers/bm25.py:57\u001b[0m, in \u001b[0;36mBM25Retriever.from_texts\u001b[0;34m(cls, texts, metadatas, ids, bm25_params, preprocess_func, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrank_bm25\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BM25Okapi\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import rank_bm25, please install with `pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank_bm25`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m texts_processed \u001b[38;5;241m=\u001b[39m [preprocess_func(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m     63\u001b[0m bm25_params \u001b[38;5;241m=\u001b[39m bm25_params \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import rank_bm25, please install with `pip install rank_bm25`."
     ]
    }
   ],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 10  # 回傳前 10 篇最相關的段落"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8873f7a-9652-40c0-9799-e01e56314705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也可以將 doc 切分成更細的 chunk 再丟給 bm25_retriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)  # ³o¸Ì¶Ç¤Jªº¬O¤@°ï page-based document\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde40e1c-9711-4b8d-8b51-d8a641380cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d905a9b-5f56-40ff-b98e-7410cd4c769b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04142730-9b6a-4b49-9cd9-d975016fafbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0cb93-6f41-4ef2-add4-73e311573032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97624ba3-85da-4bf6-a1dc-7b23e8d8c51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ba35b-23a0-467e-b75f-d47eb3e41284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31000ec4-a44d-4ed6-97b7-93fdc053c68b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
