<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Retrieval-Augmented Generation on YuChen</title><link>https://Dandelionlibra.github.io/tags/retrieval-augmented-generation/</link><description>Recent content in Retrieval-Augmented Generation on YuChen</description><generator>Hugo -- gohugo.io</generator><language>zh-Hant-TW</language><lastBuildDate>Tue, 22 Jul 2025 05:27:00 +0800</lastBuildDate><atom:link href="https://Dandelionlibra.github.io/tags/retrieval-augmented-generation/index.xml" rel="self" type="application/rss+xml"/><item><title>LightRAG 論文導讀 — Simple and Fast Retrieval-Augmented Generation 筆記</title><link>https://Dandelionlibra.github.io/post/note/lightrag-paper-review/</link><pubDate>Tue, 22 Jul 2025 05:27:00 +0800</pubDate><guid>https://Dandelionlibra.github.io/post/note/lightrag-paper-review/</guid><description>&lt;blockquote>
&lt;p>本文整理自：&lt;a class="link" href="https://arxiv.org/abs/2410.05779" target="_blank" rel="noopener"
>LightRAG: Simple and Fast Retrieval-Augmented Generation&lt;/a>&lt;br>
作者：Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang&lt;br>
發佈於 arXiv，2024年10月&lt;/p>&lt;/blockquote>
&lt;h1 id="摘要">摘要
&lt;/h1>&lt;p>《LightRAG: Simple and Fast Retrieval-Augmented Generation》提出一種輕量且高效的檢索增強生成（RAG）框架，透過結合圖結構知識與雙層檢索策略來提升大型語言模型的回應品質。現有 RAG 系統常受限於扁平資料表徵與缺乏上下文關聯，導致回答內容片段化、無法捕捉實體之間的複雜關係。為了解決這些問題，LightRAG 在文本索引和檢索過程中引入知識圖譜，並採用雙層（低階/高階）檢索架構，同時整合圖結構與向量表示以高效檢索相關實體及其關係。此框架還包含一套漸進式更新演算法，能及時融合新知識，確保模型在動態資料環境中保持有效和即時。實驗結果顯示，相較既有方法，LightRAG 在檢索準確率與效率上都有顯著提升。研究已開源並提供原始碼供社群使用。&lt;/p></description></item><item><title>Re2G 論文導讀 — Retrieve, Rerank, Generate 框架解析與應用</title><link>https://Dandelionlibra.github.io/post/note/re2g-paper-review/</link><pubDate>Tue, 22 Jul 2025 01:31:00 +0800</pubDate><guid>https://Dandelionlibra.github.io/post/note/re2g-paper-review/</guid><description>&lt;blockquote>
&lt;p>本文整理自：&lt;a class="link" href="https://arxiv.org/abs/2207.06300" target="_blank" rel="noopener"
>Re2G: Retrieve, Rerank, Generate&lt;/a>&lt;br>
作者：Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Rajaram Naik, Pengshan Cai, Alfio Gliozzo&lt;br>
發佈於 arXiv，2022年7月&lt;/p>&lt;/blockquote>
&lt;h1 id="摘要">摘要
&lt;/h1>&lt;p>大型 Transformer 模型在處理複雜任務時雖然表現強大，但在應對知識密集型任務時，仍會面臨顯著的計算成本與記憶體限制。此研究指出，非參數記憶和檢索技術能有效解決這些挑戰。&lt;br>
基於此，Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 模型應運而生，它創新地整合了神經初始檢索與重新排序機制，並以 BART 框架為基礎進行序列到序列的生成。Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 的核心創新之處在於其獨特的重新排序功能。這使得模型能夠智慧地整合來自不同檢索源的結果，即使這些源的分數不可比較，例如同時融合傳統的 BM25 算法與神經檢索（如密集段落檢索，DPR）。透過引入重排序 (Reranker) 組件，模型得以統一處理並最大化初始候選池的質量。此外，Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 還引入了&lt;strong>線上知識蒸餾方法&lt;/strong>，實現了整個檢索與生成管道的端到端訓練。這種訓練方式將系統的所有組件串接成一個整體模型，直接以最終目標作為損失函數進行優化，從而無需分階段訓練，有效提升了系統的整體性能。&lt;/p>
&lt;p>※ 在機器學習領域，「端到端訓練」（End-to-End Training）指的是將整個系統的所有組件（如檢索、重排序、生成）串接為一個整體模型，直接以最終目標（如生成正確答案）作為損失函數，反向傳播優化所有參數。這種方式不需分階段分別訓練各模組，而是讓模型自動協調各部分，最大化整體性能。&lt;/p>
&lt;hr>
&lt;h1 id="基本架構介紹">基本架構介紹
&lt;/h1>&lt;p>RAG（Retrieval-Augmented Generation）與 Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G（Retrieve, Rerank, Generate）架構模組如下圖所示：&lt;br>
&lt;img src="https://ar5iv.labs.arxiv.org/html/2207.06300/assets/x1.png"
loading="lazy"
alt="RAG 基本架構圖"
>&lt;br>
圖 1. RAG 基本架構圖：RAG 基礎流程包含檢索器與生成器，將查詢與檢索到的外部知識片段一同輸入生成模型，產生最終回答。&lt;/p>
&lt;p>&lt;img src="https://ar5iv.labs.arxiv.org/html/2207.06300/assets/x2.png"
loading="lazy"
alt="RAG 重排序架構圖"
>&lt;br>
圖 2. Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G (Retrieve, Rerank, Generate) 架構圖：進階版本在檢索後加入重排序（Rerank）模組，對候選片段進行排序優化，提升檢索結果品質與生成相關性。&lt;/p>
&lt;p>&lt;strong>RAG 架構&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>檢索器（Retriever）：從語料庫中檢索相關段落&lt;/li>
&lt;li>生成器（Generator）：基於檢索結果生成回答&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 架構&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>初始檢索層（Initial Retrieval Layer）：神經檢索 + 關鍵字檢索&lt;/li>
&lt;li>重排序層（Reranker Layer）：融合多種檢索結果並重排序&lt;/li>
&lt;li>生成層（Generation Layer）：基於重排序結果生成最終輸出&lt;/li>
&lt;/ol>
&lt;p>這種分層設計使 Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 能靈活結合多種檢索技術，並透過重排序提升候選質量，最終增強生成效果。&lt;/p>
&lt;h1 id="re2g-核心概念與創新突破">Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 核心概念與創新突破
&lt;/h1>&lt;p>Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 的兩大創新突破，分別體現在多源檢索結果的融合能力與創新的端到端訓練策略。
首先，Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 的重排序方法打破了傳統檢索分數不可比的限制，能夠有效融合來自不同檢索機制的候選結果。其次，作者提出一種改良的知識蒸餾策略，使得整個系統能僅依賴目標序列輸出進行訓練，實現從初始檢索、重排序到序列生成端到端改善。這項設計解決了當重排序器主導運算後，查詢編碼器將無法從生成損失中獲得有效梯度的問題。透過在線知識蒸餾，Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 讓重排序器成為查詢編碼器的指導，重新建立了梯度流通路，深化多模組系統間的互動訓練。&lt;/p>
&lt;p>主要流程是從大型語料庫中&lt;strong>檢索相關段落&lt;/strong>，隨後對這些初步檢索到的段落&lt;strong>進行精確的重排序&lt;/strong>，最後基於重排序後的結果&lt;strong>生成最終的輸出序列&lt;/strong>。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>檢索相關段落&lt;/strong>&lt;br>
從大型語料庫中，透過高效的初始檢索模型（如 DPR 或 BM25）篩選出與查詢相關的候選段落。&lt;br>
&lt;img src="https://ar5iv.labs.arxiv.org/html/2207.06300/assets/x4.png"
loading="lazy"
alt="雙編碼器表示模型 — 初始檢索器（DPR）"
>&lt;br>
&lt;em>圖 3：表示模型（Bi-Encoder）運作方式：查詢（Query）與段落（Passage）分別透過獨立的 BERT 編碼器轉換為向量表示（r&lt;!-- raw HTML omitted -->q&lt;!-- raw HTML omitted --> 與 r&lt;!-- raw HTML omitted -->p&lt;!-- raw HTML omitted -->），之後透過 向量內積（Inner Product） 計算相似度，作為檢索排序的依據。&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>精確的重排序&lt;/strong>&lt;br>
對初步檢索到的候選段落，使用互動模型（Reranker）進行相關性重排序。&lt;br>
此階段的關鍵創新在於能夠&lt;strong>整合多種來源的檢索結果&lt;/strong>，例如：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>BM25&lt;/strong>：基於關鍵字的傳統檢索，分數反映字詞匹配頻率&lt;/li>
&lt;li>&lt;strong>DPR&lt;/strong>：基於語意向量的神經檢索，分數來自內積相似度&lt;/li>
&lt;/ul>
&lt;p>然而，這些檢索分數性質不同，直接比對會產生偏誤。&lt;br>
為此，Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 引入 &lt;strong>獨立的 Reranker 模組&lt;/strong>，將不同檢索來源的候選進行&lt;strong>統一評分&lt;/strong>，重排序後產出可比較的結果。&lt;br>
&lt;img src="https://ar5iv.labs.arxiv.org/html/2207.06300/assets/x3.png"
loading="lazy"
alt="互動模型 — 重排序器（Reranker）"
>&lt;br>
&lt;em>圖 4：互動模型（Interaction Model）運作方式 — 查詢與段落聯合輸入 BERT，透過交叉注意力捕捉語義關聯，最終由 [CLS] 預測相關性分數。&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>基於重排序結果的生成&lt;/strong>&lt;br>
最終，將排序靠前的 Top-K 段落，與查詢共同輸入到序列生成模型（如 BART）中，生成目標回答或回應。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="re2g-模型架構詳解">Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 模型架構詳解
&lt;/h2>&lt;h3 id="初始檢索層dpr與bm25的協同作用">初始檢索層：DPR與BM25的協同作用
&lt;/h3>&lt;p>目的是建立一個包含潛在相關段落的候選池，在此階段結合了兩種互補的檢索方法：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>DPR (Dense Passage Retrieval) 密集段落檢索：DPR 採用雙編碼器表示模型，其中查詢編碼器和段落編碼器（兩者均基於 BERT）獨立生成查詢和段落的表示向量。這種獨立編碼的設計允許在檢索前預先計算語料庫中所有段落的向量，並使用近似最近鄰 (ANN) 索引。在推斷時，輸入查詢會使用 DPR 查詢編碼器進行編碼，並從 HNSW 索引中快速檢索出最相關的 Top-12 段落。  &lt;/p>
&lt;/li>
&lt;li>
&lt;p>BM25：作為一種傳統的基於關鍵字的檢索方法，BM25 擅長捕捉精確的詞彙匹配。在推斷時，查詢也會傳遞給 BM25 搜索（具體使用 Anserini 庫），並收集 Top-12 的 BM25 結果。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 在此層結合使用了 DPR (一種神經「表示模型」) 和 BM25 (一種詞彙匹配方法)。這兩種方法互補，共同產生一個初步的候選段落池，這兩種方法檢索到的段落隨後會被合併，為後續的重排序階段提供一個更全面且多樣化的候選集。&lt;/p>
&lt;blockquote>
&lt;p>舉例：&lt;br>
&lt;strong>DPR 的檢索結果：&lt;/strong>
DPR 模型會將文檔庫中的所有段落預先編碼成向量，並儲存在一個向量資料庫中，當有一個查詢進來時，DPR 會將這個查詢也編碼成一個向量，然後，它會在向量資料庫中進行「最近鄰搜索」，找出與查詢向量最相似的 K 個段落向量，這些被找出來的 K 個段落，就是 DPR 的「檢索結果」，它們是實際的文本段落。&lt;br>
例如：「尼加拉瀑布位於加拿大和美國的邊界。」&lt;/p>
&lt;p>&lt;strong>BM25 的檢索結果：&lt;/strong>
BM25 演算法會根據查詢中的關鍵字，在文檔庫中計算每個段落與查詢的相關性分數，它會根據這些分數，找出相關性最高的 K 個段落，這些被找出來的 K 個段落，就是 BM25 的「檢索結果」，它們也是實際的文本段落。&lt;br>
例如：「尼加拉瀑布是世界著名的瀑布。」&lt;/p>
&lt;p>合併指的是將這兩個獨立檢索器（DPR 和 BM25）各自找出來的候選段落列表結合起來，形成一個更大的、包含更多潛在相關段落的集合。&lt;/p>
&lt;p>例如：&lt;br>
DPR 可能檢索到段落 A, B, C, D, E。&lt;br>
BM25 可能檢索到段落 C, F, G, H, I。&lt;br>
合併後，初始候選集就可能包含 A, B, C, D, E, F, G, H, I。&lt;/p>&lt;/blockquote>
&lt;h3 id="重排序層互動模型的強大能力">重排序層：互動模型的強大能力
&lt;/h3>&lt;p>重排序層的核心目的是精煉初始檢索結果的排名，並實現來自不同檢索方法結果的合併。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>模型類型&lt;/strong>：Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 使用基於 Nogueira 和 Cho 序列對分類方法的「互動模型」進行重排序。&lt;/li>
&lt;/ul>
&lt;!-- raw HTML omitted -->
&lt;hr>
&lt;!-- raw HTML omitted -->
&lt;hr>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>機制&lt;/strong>： 在互動模型中，查詢和段落會作為一個整體輸入到 BERT 變換器中，模型會在兩個序列的所有詞元上共同應用交叉注意力機制，從而捕捉查詢和段落之間更深層次的交互關係。  &lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>優勢&lt;/strong>：透過使用互動模型對來自表示模型的 Top-N 段落進行重排序，Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 能夠同時獲得兩種模型類型的優勢：互動模型帶來的更高準確性（因其能進行更細緻的交叉注意力計算），以及表示模型帶來的可擴展性（因其允許高效的初始檢索）。這種設計模式在許多大規模資訊檢索系統中非常有效，平衡了對大型語料庫的快速初始檢索與對較小候選集的精細排名。  &lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>初始化與推斷&lt;/strong>： 重排序器從 NBoost 在 MS MARCO 數據集上訓練的 BERT 模型初始化。在推斷時，從初始檢索層合併後的段落集會傳遞給重排序器進行評分，並選出 Top-5 的段落供生成器使用。  &lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="生成層基於bart的序列生成">生成層：基於BART的序列生成
&lt;/h3>&lt;p>生成層負責根據重排序後的段落和查詢生成最終的目標輸出序列。  &lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>基礎模型&lt;/strong>：Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 使用基於 BART 的序列到序列生成模型，具體是 $BART_{LARGE}$。BART 結合了雙向編碼器（如 BERT）和自回歸解碼器（如 GPT）的優勢，使其非常適合序列到序列任務，並能有效地整合檢索到的資訊。這種選擇利用了現有的穩健生成模型，並透過外部知識增強了它們的能力。  &lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>生成器輸入與輸出&lt;/strong>：來自重排序器的 Top-5 段落會與原始查詢結合，作為 $BART_{LARGE}$ 的輸入以生成輸出。BART 生成的五個輸出序列隨後會根據重排序器分數的 softmax 進行加權，以產生最終的輸出。這個過程在 RAG 中稱為邊緣化 (marginalization)，它確保了檢索到的相關性信息能夠有效指導最終的生成。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="四階段訓練方法與線上知識蒸餾">四階段訓練方法與線上知識蒸餾
&lt;/h2>&lt;h3 id="第一階段dpr-訓練">第一階段：DPR 訓練
&lt;/h3>&lt;p>訓練數據由查詢、正向段落（來自真實標籤的來源資訊）以及「硬負向」段落（從 BM25 檢索但非真實標籤的段落）的三元組組成，這些實例會被批次處理，並且批次中其他實例的正向和硬負向段落會被用作當前實例的「批次負向」，DPR 雙編碼器模型隨後會為每個查詢提供其對正向、硬負向和批次負向段落的概率分佈。此階段的損失函數是正向段落的負對數似然，完成此階段訓練後，語料庫中的所有段落都會使用分層可導航小世界圖 (HNSW) 結合 FAISS 庫進行索引，以便後續高效檢索。&lt;/p>
&lt;h3 id="第二階段生成訓練">第二階段：生成訓練
&lt;/h3>&lt;p>此階段的訓練目標是擴展查詢編碼器的訓練，並訓練 BART&lt;!-- raw HTML omitted -->LARGE&lt;!-- raw HTML omitted --> 序列到序列模型以生成最終的目標序列輸出。這個訓練過程與 Lewis 等人描述的 RAG 模型生成訓練方法一致。&lt;/p>
&lt;h3 id="第三階段重排序訓練">第三階段：重排序訓練
&lt;/h3>&lt;p>重排序器的獨立訓練始於收集訓練集上來自 DPR 和 BM25 的初始檢索結果，這些結果隨後會被合併，並作為重排序器的訓練數據。由於某些數據集可能存在多個正向段落，因此此階段採用的損失函數是這些正向段落負對數似然之和。&lt;/p>
&lt;h3 id="第四階段端到端訓練">第四階段：端到端訓練
&lt;/h3>&lt;p>端到端訓練帶來了一個特殊挑戰：在 Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 中，重排序器的分數而非初始檢索分數用於加權每個序列在生成中的影響。這意味著重排序器可以直接透過目標輸出的真實標籤進行訓練，但查詢編碼器的梯度將為零，因為邊緣化過程不再直接依賴於查詢和段落表示向量的內積。  &lt;/p>
&lt;p>為了解決這個問題，Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 引入了一種新穎的線上知識蒸餾應用。在這種方法中，重排序器作為「教師模型」，為 DPR「學生模型」提供軟標籤。這種知識蒸餾是「線上」發生的，即在重排序器訓練的同時進行，實現了知識從一種架構（互動模型）到另一種架構（表示模型）的轉移。初始檢索（DPR）的損失函數是其在檢索到的段落上給出的概率分佈與重排序器在相同段落上給出的概率分佈之間的 KL 散度。為了平滑這些分佈、防止過度損失並穩定訓練，模型使用了溫度超參數 (T)。這種方法不僅提供了正向和負向實例的信號，還提供了「負向程度」的信號，並且能夠利用更多段落進行訓練（DPR 檢索 12 個段落，而生成只使用 Top-5），從而提供了比二元標籤更豐富的訓練信號。&lt;/p>
&lt;hr>
&lt;h2 id="實驗結果與性能分析">實驗結果與性能分析
&lt;/h2>&lt;p>此部分尚未撰寫。&lt;/p>
&lt;hr>
&lt;h2 id="結論">結論
&lt;/h2>&lt;p>Re&lt;!-- raw HTML omitted -->2&lt;!-- raw HTML omitted -->G 在槽填充、問答、事實核查和對話等任務中，無論是檢索還是端到端性能都得到了實質性提升。&lt;/p>
&lt;p>本研究的關鍵成果包括：&lt;/p>
&lt;ul>
&lt;li>重排序器的有效性。  &lt;/li>
&lt;li>線上知識蒸餾的成功。  &lt;/li>
&lt;li>多源檢索的益處。&lt;/li>
&lt;/ul>
&lt;h2 id="reference">Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://ar5iv.labs.arxiv.org/html/2207.06300#:~:text=The%20third%20solution%20is%20our,excessive%20loss%20and%20stabilize%20training" target="_blank" rel="noopener"
>Re2G: Retrieve, Rerank, Generate-ar5iv 可視化版本&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>三種 RAG 架構比較與應用解析 — Naive、Advanced、Modular RAG 差異整理</title><link>https://Dandelionlibra.github.io/post/note/rag-type-compare-note/</link><pubDate>Mon, 21 Jul 2025 09:16:00 +0800</pubDate><guid>https://Dandelionlibra.github.io/post/note/rag-type-compare-note/</guid><description>&lt;h1 id="三種-rag-技術架構比較naive-ragadvanced-rag-與-modular-rag">三種 RAG 技術架構比較：Naive RAG、Advanced RAG 與 Modular RAG
&lt;/h1>&lt;p>本文比較《Retrieval-Augmented Generation for Large Language Models: A Survey》中提出的三種檢索增強生成（RAG）技術架構：Naive RAG、Advanced RAG 和 Modular RAG。RAG 旨在結合大型語言模型（LLM）的內部知識與外部資料檢索，以提升事實正確性與時效性。這三種架構代表了 RAG 技術的演進路徑，各自引入不同模組與策略來克服先前架構的侷限。本文將從架構組成、實作方式、技術細節、應用場景與優劣比較等面向，深入剖析三類架構的差異與適用性。&lt;/p>
&lt;hr>
&lt;h2 id="架構組成與流程差異">架構組成與流程差異
&lt;/h2>&lt;h3 id="naive-rag">Naive RAG
&lt;/h3>&lt;p>最早期且基礎的 RAG 架構，僅包含索引（Indexing）、檢索（Retrieval）與生成（Generation）三個串連模組。流程為：資料向量化 → 檢索前 $K$ 個相關片段 → 查詢與檢索結果一併餵給 LLM 產生回答。此架構流程簡單、模組單一，缺乏查詢優化或反饋機制，適合快速原型開發。&lt;/p>
&lt;p>&lt;img src="https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/e0/d6/naive-rag.png"
loading="lazy"
alt="Naive RAG 架構圖"
>&lt;/p>
&lt;h3 id="advanced-rag">Advanced RAG
&lt;/h3>&lt;p>在 Naive 基礎上增加前處理與後處理模組，如查詢優化、重排序、內容過濾/壓縮等。流程仍為索引→檢索→生成，但在檢索前後插入優化步驟，提升檢索品質與生成相關性。組件包含查詢改寫、混合檢索、重排序等，能針對性強化檢索與生成階段。&lt;/p>
&lt;p>&lt;img src="https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/8f/cb/advances-rag.png"
loading="lazy"
alt="Advanced RAG 架構圖"
>&lt;/p>
&lt;h3 id="modular-rag">Modular RAG
&lt;/h3>&lt;p>最新階段，強調積木式模組化設計。除繼承前述流程外，允許多輪檢索-生成、平行資訊融合、自適應流程等。可靈活增減如網路搜尋、長程記憶、路由決策等模組，流程可重組、迭代或分支，適應複雜多變的任務需求。&lt;/p>
&lt;p>&lt;img src="https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/a7/e6/modular-rag.component.crop-16by9-m.ts=1740501066286.png/content/adobe-cms/us/en/think/topics/rag-techniques/jcr:content/root/table_of_contents/body-article-8/image_1228195012"
loading="lazy"
alt="Modular RAG 架構圖"
>&lt;/p>
&lt;hr>
&lt;h2 id="實作方式與系統特性">實作方式與系統特性
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>Naive RAG&lt;/strong>：實作最直接，僅需嵌入模型、向量資料庫與 LLM。模組線性串接，無需微調，部署維護成本低，適合簡單應用。&lt;/li>
&lt;li>&lt;strong>Advanced RAG&lt;/strong>：需引入查詢優化、重排序等模組，常用 LlamaIndex、LangChain 等框架。系統複雜度提升，需調校多個子系統，適合中等複雜度任務。&lt;/li>
&lt;li>&lt;strong>Modular RAG&lt;/strong>：高度模組化，常用流水線編排框架。每個功能獨立封裝，系統可為有向圖結構，便於擴充與維護，但開發協調成本高。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="核心技術細節">核心技術細節
&lt;/h2>&lt;h3 id="資料預處理與嵌入">資料預處理與嵌入
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>Naive RAG&lt;/strong>：文本清洗、切分、嵌入，建立向量索引，重點在語義表示。&lt;/li>
&lt;li>&lt;strong>Advanced RAG&lt;/strong>：細粒度切分、滑動視窗、metadata 標註、混合嵌入（密集+稀疏），提升檢索覆蓋率與精確性。&lt;/li>
&lt;li>&lt;strong>Modular RAG&lt;/strong>：動態資料處理，可即時抓取新資料、多模態資料、記憶模組自我增強，嵌入策略多元且可演化。&lt;/li>
&lt;/ul>
&lt;h3 id="檢索策略與查詢優化">檢索策略與查詢優化
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>Naive RAG&lt;/strong>：單輪語義相似度檢索，無查詢優化或多輪交互。&lt;/li>
&lt;li>&lt;strong>Advanced RAG&lt;/strong>：查詢重寫/擴展、多次/混合檢索、重排序與過濾，提升檢索準確率與覆蓋率。&lt;/li>
&lt;li>&lt;strong>Modular RAG&lt;/strong>：自適應多階段檢索、路由決策、平行多查詢、遞歸式檢索，根據任務動態調度檢索策略。&lt;/li>
&lt;/ul>
&lt;h3 id="上下文融合與資訊增強">上下文融合與資訊增強
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>Naive RAG&lt;/strong>：直接拼接查詢與檢索內容，無額外處理，易受雜訊干擾。&lt;/li>
&lt;li>&lt;strong>Advanced RAG&lt;/strong>：重排序、壓縮、過濾、明確引導模型引用檢索內容，提升訊息品質。&lt;/li>
&lt;li>&lt;strong>Modular RAG&lt;/strong>：多步融合、示範-搜索-預測、動態記憶、事後校驗，深度整合外部知識與模型推理。&lt;/li>
&lt;/ul>
&lt;h3 id="回答生成與控制">回答生成與控制
&lt;/h3>&lt;ul>
&lt;li>&lt;strong>Naive RAG&lt;/strong>：LLM 直接生成，控制力弱，易出現幻覺或拼貼。&lt;/li>
&lt;li>&lt;strong>Advanced RAG&lt;/strong>：提示工程、微調、反饋迴路、生成後過濾，強化可靠性與安全性。&lt;/li>
&lt;li>&lt;strong>Modular RAG&lt;/strong>：示範模組、迭代生成、後處理校驗、用戶反饋迴路，實現嚴謹的生成管控。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="適用場景與限制">適用場景與限制
&lt;/h2>&lt;ul>
&lt;li>&lt;strong>Naive RAG&lt;/strong>：適合原型、FAQ、內部知識庫等低複雜度場景，開發快但不適合高精度或多步推理任務。&lt;/li>
&lt;li>&lt;strong>Advanced RAG&lt;/strong>：適用於醫療、法律、教育等知識密集型問答，能處理較大規模知識庫，但資源需求與維護成本較高。&lt;/li>
&lt;li>&lt;strong>Modular RAG&lt;/strong>：適合大型企業、跨領域系統、需多階段推理或多源資訊整合的場景，擴展性與維護性最佳，但開發複雜度與初始成本高。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="優劣比較">優劣比較
&lt;/h2>&lt;table>
&lt;thead>
&lt;tr>
&lt;th>架構&lt;/th>
&lt;th>實用性&lt;/th>
&lt;th>可擴展性&lt;/th>
&lt;th>維護成本&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Naive RAG&lt;/td>
&lt;td>高（易用）&lt;/td>
&lt;td>低～中&lt;/td>
&lt;td>低&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Advanced RAG&lt;/td>
&lt;td>中（需專業）&lt;/td>
&lt;td>中～高&lt;/td>
&lt;td>中&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Modular RAG&lt;/td>
&lt;td>低（複雜）&lt;/td>
&lt;td>極高&lt;/td>
&lt;td>高（初始），低（局部維護）&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ul>
&lt;li>&lt;strong>Naive RAG&lt;/strong>：簡單易用、成本低，但遇到複雜任務易達天花板。&lt;/li>
&lt;li>&lt;strong>Advanced RAG&lt;/strong>：性能與複雜度平衡，適合多數專業應用，維護需專業投入。&lt;/li>
&lt;li>&lt;strong>Modular RAG&lt;/strong>：彈性與擴展性最強，適合高端需求，但開發與協調成本高。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="結論">結論
&lt;/h2>&lt;p>三種 RAG 架構各有適用場景與優劣。Naive RAG 適合快速原型與簡單應用，Advanced RAG 適合專業領域與中大型知識庫，Modular RAG 則為高複雜度、需長期演化的系統提供最佳解決方案。選擇何種架構，應根據實際需求、資源與長期維護考量權衡取捨。&lt;/p>
&lt;h2 id="reference">Reference
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://arxiv.org/abs/2312.10997" target="_blank" rel="noopener"
>Retrieval-Augmented Generation for Large Language Models: A Survey&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.thecloudgirl.dev/blog/three-paradigms-of-retrieval-augmented-generation-rag-for-llms#:~:text=,on%20embeddings%20from%20language%20models" target="_blank" rel="noopener"
>Three Paradigms of Retrieval-Augmented Generation (RAG) for LLMs&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.ibm.com/think/topics/rag-techniques#:~:" target="_blank" rel="noopener"
>RAG Techniques | IBM Think&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>檢索增強大型語言模型綜述 — RA-LLMs 系統性回顧與應用解析</title><link>https://Dandelionlibra.github.io/post/note/rag-llms-survey-note/</link><pubDate>Sun, 20 Jul 2025 08:50:00 +0800</pubDate><guid>https://Dandelionlibra.github.io/post/note/rag-llms-survey-note/</guid><description>&lt;blockquote>
&lt;p>本文整理自：&lt;a class="link" href="https://arxiv.org/abs/2405.06211" target="_blank" rel="noopener"
>A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models&lt;/a>&lt;br>
作者：Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li&lt;br>
發佈於 arXiv，2024年5月&lt;/p>&lt;/blockquote>
&lt;h1 id="摘要">摘要
&lt;/h1>&lt;p>大型語言模型（LLMs）雖展現強大生成能力，但受限於內部知識與幻覺問題。檢索增強生成（Retrieval-Augmented Generation;RAG）透過即時檢索外部資訊，提升回應的可靠性與時效性。本文整理 RA-LLMs 的架構、訓練策略與應用，並探討其面臨的挑戰與未來發展，展現檢索對提升 LLM 實用性的關鍵價值。&lt;/p>
&lt;h1 id="前言">前言
&lt;/h1>&lt;p>檢索增強生成（RAG）技術透過結合資訊檢索與大型語言模型（LLMs），補足模型知識不足與幻覺問題，近年受到廣泛關注。LLMs 雖具強大生成能力，卻常受限於知識時效與專領域應用，而 RA-LLMs 則透過檢索外部資料提升生成品質。&lt;/p>
&lt;h1 id="背景">背景
&lt;/h1>&lt;h2 id="大型語言模型">大型語言模型
&lt;/h2>&lt;ul>
&lt;li>
&lt;p>應用&lt;/p>
&lt;ul>
&lt;li>在特定資料集上進行微調，LLM 可以適應各種下游任務，使其能夠專注於特定領域或應用。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>架構:&lt;/p>
&lt;ul>
&lt;li>Encoder-only 模型，雙向編碼，可同時考慮單詞左右語境。&lt;br>
ex. BERT&lt;/li>
&lt;li>Decoder-only 模型，單向生成（左至右），根據前文預測下個字元。&lt;br>
ex. GPT&lt;/li>
&lt;li>Encoder-Decoder 模型，將輸入編碼後，再由解碼器生成對應輸出。&lt;br>
ex. T5&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="提示學習">提示學習
&lt;/h2>&lt;h3 id="提示工程prompt-engineering">提示工程（Prompt Engineering）
&lt;/h3>&lt;p>因為 LLM 的參數量通常非常龐大，因此提示學習的發展可使 LLM 不需為了特定任務進行大量微調，就可以實現各項任務。&lt;br>
缺點：當缺乏專業領域知識時，生成結果可能不夠精確。&lt;/p>
&lt;h3 id="上下文學習in-context-learning-icl">上下文學習（In-Context Learning, ICL）
&lt;/h3>&lt;p>為提示學習的一種形式，透過在提示中提供範例示範，讓 LLM 觀察並學習任務模式。&lt;br>
缺點：成效高度依賴範例品質、當缺乏必要知識或資訊時，可能導致生成結果不理想。&lt;/p>
&lt;p>為克服這些問題，發展出 RAG（檢索增強生成）技術，RAG 結合檢索與生成，提升 LLM 在多任務中的表現與適應性。&lt;/p>
&lt;h1 id="內文">內文
&lt;/h1>&lt;p>LLMs 時代的 RAG 架構大致包含檢索、產生和增強三個主要流程。&lt;/p>
&lt;p>&lt;img src="https://ar5iv.labs.arxiv.org/html/2405.06211/assets/x2.png"
loading="lazy"
alt="RAG 系統總覽"
>&lt;br>
&lt;em>圖 1：RAG 系統總覽，涵蓋檢索、生成與增強三大流程。來源：原論文&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://ar5iv.labs.arxiv.org/html/2405.06211/assets/x3.png"
loading="lazy"
alt="RAG 系統細節流程"
>&lt;br>
&lt;em>圖 2：RAG 在 RA-LLM 中的流程圖，展示各模組間的互動。來源：原論文&lt;/em>&lt;/p>
&lt;h2 id="retrieval">Retrieval
&lt;/h2>&lt;p>RAG 旨在從外部知識源提供關鍵訊息給 LLM。&lt;/p>
&lt;h3 id="retriever-type">Retriever Type
&lt;/h3>&lt;p>依照資訊編碼區分。&lt;/p>
&lt;ul>
&lt;li>稀疏檢索（Sparse Retrieval）：直接匹配詞彙並依頻率排名。&lt;/li>
&lt;li>稠密檢索（Dense Retrieval）：將查詢與文檔嵌入為向量，透過語意相似度檢索。&lt;/li>
&lt;/ul>
&lt;h3 id="retrieval-granularity">Retrieval Granularity
&lt;/h3>&lt;p>檢索單位的選擇對效能與計算成本有重大影響。&lt;/p>
&lt;ul>
&lt;li>Chunk(passages): 包含緊湊且完整的資訊，冗餘和不相關性較少，是 RAG 中主流的檢索文本粒度。&lt;/li>
&lt;li>Token: 實現更快的搜尋，但會給資料庫儲存帶來更多負擔。適用於需要稀有模式或領域外資料的情況。&lt;/li>
&lt;li>Entity: 實體檢索是從知識而非語言的角度檢索，對於以實體為中心的任務更有效，並且與詞元檢索相比，在空間上更高效。&lt;/li>
&lt;/ul>
&lt;h3 id="retrieval-enhancement-strategies">Retrieval Enhancement strategies
&lt;/h3>&lt;ul>
&lt;li>
&lt;p>檢索前優化（Pre-retrieval）&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Query Expansion (查詢擴展)：透過加入相關詞彙或概念來擴大原始查詢的範圍。例如，利用大型語言模型 (LLM) 生成偽文件，並從中提取相關資訊來擴展查詢，有助於消除歧義並引導檢索器。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Query Rewrite (查詢重寫)：旨在重新彌合原始查詢，使其更適合檢索。這可能涉及澄清查詢意圖、使其更精確，或是將其轉換為檢索功能更容易理解的形式。例如，利用 LLM 將原始問題重寫為更利於檢索的版本。&lt;br>
舉例，&lt;strong>多次&lt;/strong>詢問模型他的檢索資料是否正確。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Query Augmentation (查詢增強)：將原始查詢與初步生成的內容結合，形成一個新的查詢。這種策略可以增加查詢與潛在相關文件之間的詞彙和語義重疊，有助於檢索出更多有助於答案生成的資訊。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>檢索後優化（Post-retrieval）&lt;/p>
&lt;ul>
&lt;li>
&lt;p>重排序與過濾&lt;br>
對檢索到的文件進行重新排序，將最相關的資訊排在前面，並過濾掉不相關或低品質的文件。例如，透過不同的檢索方法組裝文件並進行重排序，以提升檢索結果的穩健性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>雜訊過濾與整合&lt;br>
處理檢索到的資訊中可能存在的雜訊或不相關內容，以避免其對生成過程產生負面影響。同時，將清洗過的資訊有效地整合進生成模型。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>壓縮與摘要&lt;br>
針對檢索到的長篇文件，進行壓縮或生成摘要，以解決大型語言模型輸入長度限制的問題。例如，將檢索到的文件處理成文本摘要，再用於模型生成。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="database">Database
&lt;/h3>&lt;p>RA-LLM 的檢索資料庫可為封閉式或開放式來源。&lt;/p>
&lt;ul>
&lt;li>封閉式資料庫:
通常以鍵值對 (key-value pairs) 的形式儲存知識。&lt;/li>
&lt;li>開放式資料庫:
利用搜尋引擎（如 Bing、Google）獲取即時資訊。&lt;/li>
&lt;/ul>
&lt;h2 id="生成generation">生成（Generation）
&lt;/h2>&lt;p>生成模組的設計高度依賴於下游任務需求，因而得以適應不同的任務需求。&lt;/p>
&lt;h3 id="可調參數生成器白箱parameter-accessible-generators">可調參數生成器（白箱，Parameter-Accessible Generators）
&lt;/h3>&lt;ul>
&lt;li>
&lt;p>Encoder-Decoder&lt;/p>
&lt;ul>
&lt;li>擁有獨立的編碼器（Encoder）與解碼器（Decoder），分別處理輸入與生成的目標。&lt;/li>
&lt;li>Encoder 先將輸入編碼為上下文表示，Decoder 以 Cross-Attention 讀取 Encoder 輸出，逐步生成結果。&lt;/li>
&lt;li>模型的目標是「根據編碼後的輸入與先前生成的結果，預測下一個 token」。&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>輸入：請介紹 Transformer。
Encoder 編碼後 → [內部上下文表示]
Decoder 讀取表示 → 生成：Transformer 是一種...
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Decoder-only&lt;/p>
&lt;ul>
&lt;li>沒有獨立的編碼器。&lt;/li>
&lt;li>輸入（如問題、提示） 和 目標（要生成的內容） 會被串接成同一序列，並從左到右進行處理。&lt;/li>
&lt;li>模型的目標是學會「根據前面內容，預測下一個 token」。&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>輸入：請介紹 Transformer。
模型看到的內容：請介紹 Transformer。&amp;lt;接著是生成的回答...&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;h3 id="不可調參數生成器黑箱parameter-inaccessible-generators">不可調參數生成器（黑箱，Parameter-Inaccessible Generators）
&lt;/h3>&lt;p>無法直接修改模型本身，且難以進行微調，因此更側重於優化檢索和增強的過程。它們的目標是透過為輸入 (prompts) 提供更優質的知識、指導或範例來增強 Generator 的性能。&lt;/p>
&lt;h2 id="增強augmentation">增強（Augmentation）
&lt;/h2>&lt;h3 id="input-layer-integration">Input-Layer Integration:
&lt;/h3>&lt;ul>
&lt;li>
&lt;p>&lt;strong>串聯整合&lt;/strong>：如 In-Context RALM (Ram et al., 2023)，將原始輸入與所有檢索文件串聯為單一序列輸入生成模型。&lt;/p>
&lt;ul>
&lt;li>問題：輸入長度易超過模型處理上限，需移除部分詞元，可能導致資訊遺失。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>平行整合&lt;/strong>：如 FID (Izacard and Grave, 2021b)、Atlas、REPLUG，將每個檢索文件獨立編碼，僅在後續步驟聚合結果。&lt;/p>
&lt;ul>
&lt;li>優點：更能擴展至大量上下文，減少資訊丟失風險。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>通常，大多數基於黑盒生成的 RAG 方法都採用此法，因為生成模型的中間層和輸出分佈都無法存取。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="output-layer-integration">Output-Layer Integration:
&lt;/h3>&lt;p>一種後處理 (post-hoc) 的檢索增強方式，它不直接干預生成模型的內部運作或其生成過程，而是在模型產生初步結果之後，才將檢索到的資訊與這些結果進行結合。&lt;/p>
&lt;h3 id="intermediate-layer-integration">Intermediate-Layer Integration:
&lt;/h3>&lt;p>在生成模型內部的中間層注入檢索資訊，相較於輸入層與輸出層整合，屬於 半參數式（Semi-parametric） 的強化方式，具有更高的資訊融合深度與潛力。&lt;/p>
&lt;h3 id="34retrieval-augmentation-necessity-and-frequency">3.4.Retrieval Augmentation Necessity and Frequency
&lt;/h3>&lt;p>基於 LLM 的生成中，檢索操作旨在補充知識以增強生成。儘管檢索增強模型前景光明，但若不加區分地使用不相關的段落進行增強，可能會覆蓋 LLM 已有的正確知識，導致錯誤回應，甚至使幻覺率翻倍。因此，對於檢索增強型 LLM (RA-LLMs) 來說，&lt;strong>準確回憶先驗知識並僅在必要時選擇性地整合檢索資訊&lt;/strong>至關重要，這是實現穩健 RA-LLMs 的關鍵。&lt;/p>
&lt;h4 id="檢索必要性判斷">檢索必要性判斷
&lt;/h4>&lt;p>大多數方法基於 LLM 的初步答案或內部推理結果來判斷是否需要檢索：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>特殊標記控制&lt;/strong>：如 Self-RAG，引入特殊標記評估檢索必要性並控制行為。&lt;/li>
&lt;li>&lt;strong>迭代提示決策&lt;/strong>：設計迭代提示決定生成中是否需要額外資訊。&lt;/li>
&lt;li>&lt;strong>基於信賴度 (Logits Confidence)&lt;/strong>：傳統 RAG 中透過評估生成模型輸出的 logits 信賴度來判斷。如 FLARE，當 logits 低於閾值時動態觸發 RAG。&lt;/li>
&lt;li>&lt;strong>協同檢測&lt;/strong>：如 SlimPLM，利用輕量級代理模型生成「啟發式答案」檢測 LLM 缺失知識，並用於查詢重寫以促進檢索。&lt;/li>
&lt;/ul>
&lt;h4 id="檢索頻率-retrieval-frequency">檢索頻率 (Retrieval Frequency)
&lt;/h4>&lt;p>檢索頻率（或稱檢索步長）是決定生成過程中檢索使用程度的重要設計考量，影響模型的效率和有效性。當不考慮檢索必要性時，檢索頻率通常是預定義和固定的，主要有三種設定：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>一次性檢索 (One-time retrieval)&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>方式&lt;/strong>：在生成過程開始時只調用一次檢索功能，檢索所有所需資訊，然後提供給生成模型。&lt;/li>
&lt;li>&lt;strong>適用場景&lt;/strong>：外部資料庫資訊需求對 LLM 來說很明確的情況。&lt;/li>
&lt;li>&lt;strong>限制&lt;/strong>：對於需要長篇輸出的任務（如開放域摘要），預先檢索的文件可能不足以支持整個生成序列，需要生成中進行檢索操作。&lt;/li>
&lt;li>&lt;strong>範例&lt;/strong>：REALM。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>每 N 個詞元檢索 (Every-n-token retrieval)&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>方式&lt;/strong>：在生成過程中每隔 N 個詞元觸發一次檢索。&lt;/li>
&lt;li>&lt;strong>適用場景&lt;/strong>：需要持續資訊補充的長篇生成任務。&lt;/li>
&lt;li>&lt;strong>範例&lt;/strong>：In-Context RALM、RETRO。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>每個詞元檢索 (Every-token retrieval)&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>方式&lt;/strong>：在生成過程中，為每個詞元的預測都檢索資訊。&lt;/li>
&lt;li>&lt;strong>頻率&lt;/strong>：最頻繁的檢索策略。&lt;/li>
&lt;li>&lt;strong>範例&lt;/strong>：kNN-LM。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>權衡&lt;/strong>：
總體而言，檢索頻率影響 RAG 方法的有效性和效率。更頻繁的檢索通常帶來更好的性能，但也顯著增加計算成本。&lt;/p>
&lt;h2 id="ra-llms-訓練策略概述">RA-LLMs 訓練策略概述
&lt;/h2>&lt;p>&lt;img src="https://ar5iv.labs.arxiv.org/html/2405.06211/assets/x4.png"
loading="lazy"
alt="RA-LLMs 訓練策略總覽"
>&lt;br>
&lt;em>圖 3：RA-LLMs 訓練策略總覽，涵蓋免訓練與需訓練方法。來源：原論文&lt;/em>&lt;/p>
&lt;h3 id="免訓練方法training-free">免訓練方法（Training-Free）
&lt;/h3>&lt;h4 id="prompt-engineering-based-methods">Prompt Engineering-based Methods
&lt;/h4>&lt;p>將檢索到的外部知識，直接整合進 LLM 的提示（Prompt），作為上下文輔助模型生成。&lt;br>
舉例，In-Context RALM 在不改動 LLM 參數的情況下，直接將檢索到的文件插入於原始提示之前，增強生成過程。IRCoT 則交錯進行 chain-of-thought（CoT）生成與知識檢索步驟，使每一步推理都能檢索到更相關的資訊。GENREAD 不是從大型語料庫檢索知識，而是先讓 LLM 根據查詢生成上下文文件，再根據這些上下文與問題產生答案。SKR 則引導 LLM 判斷是否能僅依靠內部知識回答問題，若不足再選擇性調用檢索器，靈活結合內外部知識。TOC 針對模糊問題，先檢索相關知識，並遞迴將問題拆解為多個明確子問題，最終聚合生成長篇答案。&lt;/p>
&lt;ul>
&lt;li>特點：
&lt;ul>
&lt;li>無需模型訓練&lt;/li>
&lt;li>靠設計合理的提示與檢索流程提升效果&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="retrieval-guided-token-generation-methods">Retrieval-Guided Token Generation Methods
&lt;/h4>&lt;p>透過檢索結果來調整 LLM 的 Token 預測分布，影響每一步的生成。&lt;br>
舉例，例如 KNN-LMs 會根據當前查詢從資料庫檢索出 k 個最相關的上下文，計算鄰近分布，並將其與原模型的輸出分布進行插值校正，以提升生成結果的準確性。Rest 則以非參數檢索資料庫取代傳統的參數式草稿模型，根據當前上下文檢索相關 token，輔助推測式生成（speculative decoding）。&lt;/p>
&lt;ul>
&lt;li>特點：
&lt;ul>
&lt;li>不更改模型權重&lt;/li>
&lt;li>通常作為後處理或推測性生成的輔助&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>※ 這兩類免訓練方法，分別著重於：&lt;/p>
&lt;ul>
&lt;li>提示工程 — 調整輸入&lt;/li>
&lt;li>生成控制 — 調整輸出過程&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="需訓練方法training-based">需訓練方法（Training-Based）
&lt;/h3>&lt;ol>
&lt;li>Independent Training&lt;br>
獨立訓練方法會將 RAG 流程中的每個組件分開、獨立地進行訓練，這意味著在訓練過程中，這兩個組件之間沒有任何交互作用。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>目的與優勢：&lt;/strong>&lt;br>
相較於無需訓練的方法，獨立訓練能有效提升 RAG 模型的性能。&lt;br>
1. 訓練 LLMs 以更好地利用檢索到的知識。&lt;br>
2. 訓練檢索器以彌合資訊檢索與語言生成之間的差距。&lt;/p>
&lt;p>&lt;strong>檢索器類型：&lt;/strong>
* 稀疏檢索器 (Sparse Retriever) ：
這類檢索器通常利用稀疏特徵，例如詞頻，來表示文件，並根據任務特定的指標（如 TF-IDF 和 BM25）計算相關性分數 。&lt;/p>
&lt;pre>&lt;code>* 密集檢索器 (Dense Retriever) ：
密集檢索器則採用深度神經網絡將查詢和文件編碼成密集表示 (dense representations)。然後，通常使用內積計算相關性分數並檢索相關的外部知識。序列訓練透過協調訓練的方式，尋求更深層次的整合效果。
&lt;/code>&lt;/pre>
&lt;ol start="2">
&lt;li>
&lt;p>Sequential Training&lt;br>
序列訓練方法則採取分階段的訓練方式。首先訓練一個模組（例如檢索器），然後再利用這個訓練好的模組去指導另一個模組（例如生成器）的調整過程，目的在改善模組間的協同作用。&lt;/p>
&lt;p>&lt;strong>訓練流程：&lt;/strong> 序列訓練通常分為兩個階段&lt;/p>
&lt;ol>
&lt;li>**初始預訓練：**首先對檢索器或生成器中的一個模組進行獨立的預訓練。&lt;/li>
&lt;li>**固定與訓練：**一旦其中一個模組完成預訓練，它就會被固定（freeze）下來，而另一個模組則在其輔助下進行訓練。&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>優勢與靈活性：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>協同增益：與獨立訓練相比，序列訓練的優勢在於可訓練的模組能夠受益於固定模組的引導和協助，從而更好地適應彼此。&lt;/li>
&lt;li>利用現有模型：值得注意的是，許多已經預訓練好的強大模型（例如 BERT、CLIP、T5）可以直接作為固定模組使用，從而省略了初始的預訓練步驟，進一步提高了效率。&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>根據檢索器和生成器之間的訓練順序，序列訓練可分為兩大類：&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>檢索器優先 (Retriever First)：&lt;br>
此類方法首先訓練檢索器，然後將其固定，再訓練生成器。&lt;/li>
&lt;li>LLMs 優先 (LLMs First)：&lt;br>
此類方法則相反，先訓練 LLM，再將其固定，然後訓練檢索器。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Joint Training&lt;br>
聯合訓練方法則是同時訓練檢索器和生成器，這種方式是為了讓兩個模組在訓練過程中相互協調、共同進步。&lt;/p>
&lt;/li>
&lt;/ol></description></item></channel></rss>