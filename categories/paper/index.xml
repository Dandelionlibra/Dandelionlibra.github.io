<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Paper on YuChen</title><link>https://Dandelionlibra.github.io/categories/paper/</link><description>Recent content in Paper on YuChen</description><generator>Hugo -- gohugo.io</generator><language>zh-Hant-TW</language><lastBuildDate>Tue, 22 Jul 2025 05:27:00 +0800</lastBuildDate><atom:link href="https://Dandelionlibra.github.io/categories/paper/index.xml" rel="self" type="application/rss+xml"/><item><title>LightRAG 論文導讀 — Simple and Fast Retrieval-Augmented Generation 筆記</title><link>https://Dandelionlibra.github.io/post/paper/lightrag-paper-review/</link><pubDate>Tue, 22 Jul 2025 05:27:00 +0800</pubDate><guid>https://Dandelionlibra.github.io/post/paper/lightrag-paper-review/</guid><description>&lt;blockquote&gt;
&lt;p&gt;本文整理自：&lt;a class="link" href="https://arxiv.org/abs/2410.05779" target="_blank" rel="noopener"
&gt;LightRAG: Simple and Fast Retrieval-Augmented Generation&lt;/a&gt;&lt;br&gt;
作者：Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang&lt;br&gt;
發佈於 arXiv，2024年10月&lt;/p&gt;&lt;/blockquote&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;RAG 透過整合外部知識來源，提升 LLMs 回應的準確性與上下文相關性，但面臨&lt;strong&gt;過度依賴平面資料表示&lt;/strong&gt; (flat data representations)、&lt;strong&gt;上下文感知能力不足&lt;/strong&gt; (inadequate contextual awareness)、&lt;strong&gt;導致生成碎片化答案&lt;/strong&gt; (fragmented answers)，無法捕捉複雜的相互依賴關係 (inter-dependencies)。&lt;br&gt;
LightRAG，提出透過將圖結構 (graph structures) 引入文本的索引 (text indexing) 和檢索 (retrieval) 過程來解決上述問題。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id="引言"&gt;引言
&lt;/h1&gt;&lt;h2 id="現有-rag-系統的局限性"&gt;現有 RAG 系統的局限性
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;依賴平面資料表示：&lt;/strong&gt; 許多方法依賴於平面資料表示（flat data representations），限制了它們根據實體之間複雜關係來理解和檢索資訊的能力。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;缺乏上下文感知：&lt;/strong&gt; 這些系統通常缺乏維持不同實體及其相互關係之間連貫性所需的上下文感知能力，導致回應可能無法完全解決用戶查詢。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;例：考慮用戶提問「電動車的興起如何影響城市空氣品質和大眾運輸基礎設施？」現有 RAG 方法可能檢索到關於電動車、空氣污染和公共交通挑戰的獨立文檔，但難以將這些信息綜合為一個連貫的回應。它們可能無法解釋電動車的普及如何改善空氣品質，進而可能影響公共交通規劃，用戶可能收到一個碎片化的答案，未能充分捕捉這些主題之間複雜的相互依賴關係。&lt;/p&gt;&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="lightrag-模型概述"&gt;LightRAG 模型概述
&lt;/h2&gt;&lt;p&gt;增強了系統捕捉實體之間複雜相互依賴關係的能力，從而產生更連貫和上下文更豐富的回應。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;高效雙層檢索策略：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;低層次檢索（low-level retrieval）： 側重於關於特定實體及其關係的精確資訊。&lt;/li&gt;
&lt;li&gt;高層次檢索（high-level retrieval）： 涵蓋更廣泛的主題和概念。&lt;/li&gt;
&lt;li&gt;優勢： 透過結合詳細和概念性檢索，LightRAG 有效適應多樣化的查詢範圍，確保用戶收到符合其特定需求的相關且全面的回應。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;圖結構與向量表示的整合：&lt;/strong&gt; 透過將圖結構與向量表示整合在一起，本 LightRAG 促進了相關實體和關係的高效檢索，同時透過從所構建的知識圖中獲取相關結構信息，增強了結果的全面性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="本研究在-rag-系統中的關注點"&gt;本研究在 RAG 系統中的關注點
&lt;/h2&gt;&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;全面信息檢索&lt;/strong&gt; (Comprehensive Information Retrieval)： 索引功能 ϕ(⋅) 必須善於提取全局信息，這對於增強模型有效回答查詢的能力至關重要。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;高效且低成本檢索&lt;/strong&gt; (Efficient and Low-Cost Retrieval)： 索引化的資料結構 𝒟^ 必須能夠實現快速且具成本效益的檢索，以有效處理高容量的查詢。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;快速適應數據變化&lt;/strong&gt; (Fast Adaptation to Data Changes)： 能夠迅速有效地調整數據結構以整合來自外部知識庫的新信息，這對於確保系統在不斷變化的信息環境中保持更新和相關性至關重要。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id="內文"&gt;內文
&lt;/h1&gt;&lt;h2 id="lightrag-框架的整體架構"&gt;LightRAG 框架的整體架構
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/refs/heads/main/README.assets/b2aaf634151b4706892693ffb43d9093.png"
loading="lazy"
alt="LightRAG 框架總覽"
&gt;&lt;br&gt;
&lt;em&gt;圖 1. LightRAG 框架總覽（取自原論文）&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;架構如圖 1 所示。&lt;/p&gt;
&lt;p&gt;流程從&lt;strong&gt;原始文本塊&lt;/strong&gt;開始，這些文本塊首先透過&lt;strong&gt;基於圖形的文本索引&lt;/strong&gt;（Graph-based Text Indexing）階段進行處理，過程包含幾個關鍵子組件：&lt;strong&gt;實體與關係提取&lt;/strong&gt;（Entity &amp;amp; Rel Extraction）、&lt;strong&gt;LLM 剖析&lt;/strong&gt;（LLM Profiling）和&lt;strong&gt;去重&lt;/strong&gt;（Deduplication），最後的輸出是一個用於檢索的&lt;strong&gt;索引圖&lt;/strong&gt;（Index Graph）。&lt;br&gt;
接著，Query LLM 接收輸入查詢，並從中生成&lt;strong&gt;低層級關鍵字&lt;/strong&gt;（Low-level Keys，包括實體和關係）和&lt;strong&gt;高層級關鍵字&lt;/strong&gt;（High-level Keys，包括語境和原始文本塊）。這些關鍵字隨後被送入&lt;strong&gt;雙層級檢索範式&lt;/strong&gt;（Dual-level Retrieval Paradigm），此範式與「索引圖」和「原始文本塊」互動，以檢索相關資訊。最終，檢索到的資訊被傳回 Query LLM 進行檢索增強的答案生成（Retrieval-Augmented Answer Generation）。  
圖中展示了以「索引圖」作為核心儲存庫，這張圖不僅用來整理新資訊（索引），也用來尋找資訊（檢索），這代表系統不再只是儲存一堆零散的文字片段，而是將知識組織成一個有結構的網路，能更智慧地找出事物之間的關聯。&lt;br&gt;
此外，處理查詢的 LLM 在 LightRAG 多次出現，它不只負責生成最終答案，還會參與理解問題、引導系統去尋找相關資訊，並將找到的資料整合起來。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="基於圖形的文本索引"&gt;基於圖形的文本索引
&lt;/h2&gt;&lt;p&gt;LightRAG 透過將文件分割成更小、更易於管理的片段來增強檢索系統。這種策略允許快速識別和存取相關資訊，而無需分析整個文件 。隨後，系統利用大型語言模型（LLMs）來識別和提取各種實體（例如，名稱、日期、位置和事件）以及它們之間的關係 。透過這個過程收集到的資訊將用於創建一個全面的知識圖譜，突顯整個文件集合中的連結和見解。&lt;/p&gt;
&lt;p&gt;圖形生成模組正式表示為：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;實體與關係提取 (R(⋅))&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;將文檔切分為更小的片段，方便快速檢索與處理。&lt;/li&gt;
&lt;li&gt;使用大型語言模型抽取實體（如人名、地點、事件）以及它們之間的關係，構建知識圖譜。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LLM 剖析以生成鍵值對 (P(⋅))&lt;/strong&gt;：為每個實體與關係生成索引鍵（key）與對應摘要文本（value），形成文本鍵值對（K, V）。實體通常採用名稱作為鍵；關係也由關聯實體摘要或主題語形成鍵。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;去重以優化圖操作 (D(⋅))&lt;/strong&gt;：合併相同的實體與關係，以減少圖的複雜度，優化後續運算效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;優勢&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;全面性理解：透過多跳子圖 (multi-hop subgraphs) 強化對文本中跨關係依賴的理解。&lt;/li&gt;
&lt;li&gt;檢索效率提升：採用鍵值對結構提升查詢精準度與速度，相較於傳統依賴 chunk traversal 方法更具效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;增量更新 (Incremental Knowledge Base Update)&lt;/strong&gt;：新文檔插入時，只對該文檔進行索引解析，並與原有圖進行合併，無需重建整個索引，極大降低計算成本並提升更新速度。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="雙層次檢索機制-dual-level-retrieval-paradigm"&gt;雙層次檢索機制 (Dual-level Retrieval Paradigm)
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;分類查詢類型&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Specific Queries：查找特定實體的資訊。&lt;br&gt;
ex. 誰寫了《Pride and Prejudice》？&lt;/li&gt;
&lt;li&gt;Abstract Queries：探索概念性主題。&lt;br&gt;
ex. 人工智慧如何影響現代教育？&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;strong&gt;查詢關鍵詞抽取 (Keyword Extraction)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;將查詢分為：
&lt;ul&gt;
&lt;li&gt;低階關鍵詞（Low‑level）——聚焦具體實體或關係（例：人名、事件）。&lt;/li&gt;
&lt;li&gt;高階關鍵詞（High‑level）——概括性主題或概念（例：制度變革趨勢）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;低階檢索 (Low‑Level Retrieval)：透過低階鍵精確定位實體與其屬性或鄰近關係。&lt;/li&gt;
&lt;li&gt;高階檢索 (High‑Level Retrieval)：透過高階鍵尋找涵蓋廣泛主題或總覽資訊。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;strong&gt;圖結構與向量結合檢索&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;結合知識圖結構與向量表示（vector embeddings），在進行查詢時同時考量局部（local）與全局（global）語義。&lt;/li&gt;
&lt;li&gt;並引入高階相關結構資訊（如一跳鄰居）加強檢索結果的完整性與關聯度。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="檢索增強答案生成"&gt;檢索增強答案生成
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;使用檢索回來的資料，將所有相關的實體與關係摘要（由 profiling function 生成；P(⋅)）作為 LLM 的上下文輸入。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;結合查詢與上下文生成回答，將查詢緊接相關資料餵給 LLM，生成上下文合宜、符合需求的回答。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="複雜度分析"&gt;複雜度分析
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;索引階段：對文本進行實體與關係抽取時，需對每個文本片段呼叫一次 LLM，不增加額外系統負擔。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;檢索階段：相較於傳統高成本的 GraphRAG 社群遍歷，LightRAG 採用向量搜索與圖結構結合方式──只需一次 API 呼叫與少量 token，即完成檢索。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id="結論重點整理"&gt;結論重點整理
&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;圖結構索引（Graph-based Indexing）&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;以實體與關係為核心建構知識圖，支援去重與增量更新，不必重建全索引。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;雙層檢索（Dual-level Retrieval）&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;低階檢索：精確定位實體與細節資訊。&lt;/li&gt;
&lt;li&gt;高階檢索：捕捉抽象主題與全局脈絡。&lt;/li&gt;
&lt;li&gt;兩者結合確保 全面性 + 精準性。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;向量與圖結合（Hybrid Retrieval）&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;結合向量相似度與多跳圖結構檢索，兼顧語義相關與結構關聯。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="4"&gt;
&lt;li&gt;低成本高效率&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;檢索階段僅需一次 API 呼叫、百級 token，較 GraphRAG 大幅節省計算與金錢成本。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;動態適應性&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;能即時合併新知識圖節點，適合動態更新的大型知識庫（如法律、醫療、科研）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;LightRAG vs GraphRAG&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;面向&lt;/th&gt;
&lt;th&gt;LightRAG&lt;/th&gt;
&lt;th&gt;GraphRAG&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;索引結構&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;基於 &lt;strong&gt;圖結構（Knowledge Graph）+ 向量索引&lt;/strong&gt;，以實體與關係為鍵值對（Key-Value）存儲，支援去重與增量更新&lt;/td&gt;
&lt;td&gt;基於 &lt;strong&gt;圖結構社群（Graph Community）&lt;/strong&gt;，以社群摘要為檢索單位&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;檢索策略&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;雙層檢索&lt;/strong&gt;：低階（細節）+ 高階（主題）並結合多跳圖檢索與向量相似度&lt;/td&gt;
&lt;td&gt;單層檢索：依社群摘要進行檢索，缺乏細粒度與多層結合&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;生成過程&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;檢索到的實體與關係摘要直接送入 LLM，結構化輸入更精準&lt;/td&gt;
&lt;td&gt;將社群摘要送入 LLM，依社群內容生成答案&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;檢索成本&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;一次 API 呼叫&lt;/td&gt;
&lt;td&gt;多次 API 呼叫&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;增量更新&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;支援 &lt;strong&gt;快速合併更新&lt;/strong&gt;，僅更新新文檔的圖索引&lt;/td&gt;
&lt;td&gt;需重建整個社群報告，成本高&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;資訊完整性&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;低階檢索補足細節，高階檢索抓全局，全面性佳&lt;/td&gt;
&lt;td&gt;依賴社群摘要，可能遺漏細節&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;適用場景&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;資料頻繁更新、大型知識庫、多層次查詢需求&lt;/td&gt;
&lt;td&gt;資料相對靜態、偏向高層總覽查詢&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;hr&gt;
&lt;h1 id="reference"&gt;Reference
&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://ar5iv.labs.arxiv.org/html/2410.05779" target="_blank" rel="noopener"
&gt;LightRAG: Simple and Fast Retrieval-Augmented Generation-ar5iv 可視化版本&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Re2G 論文導讀 — Retrieve, Rerank, Generate 框架解析與應用</title><link>https://Dandelionlibra.github.io/post/paper/re2g-paper-review/</link><pubDate>Tue, 22 Jul 2025 01:31:00 +0800</pubDate><guid>https://Dandelionlibra.github.io/post/paper/re2g-paper-review/</guid><description>&lt;blockquote&gt;
&lt;p&gt;本文整理自：&lt;a class="link" href="https://arxiv.org/abs/2207.06300" target="_blank" rel="noopener"
&gt;Re2G: Retrieve, Rerank, Generate&lt;/a&gt;&lt;br&gt;
作者：Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Rajaram Naik, Pengshan Cai, Alfio Gliozzo&lt;br&gt;
發佈於 arXiv，2022年7月&lt;/p&gt;&lt;/blockquote&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;大型 Transformer 模型在處理複雜任務時雖然表現強大，但在應對知識密集型任務時，仍會面臨顯著的計算成本與記憶體限制。此研究指出，非參數記憶和檢索技術能有效解決這些挑戰。&lt;br&gt;
基於此，Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 模型應運而生，它創新地整合了神經初始檢索與重新排序機制，並以 BART 框架為基礎進行序列到序列的生成。Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 的核心創新之處在於其獨特的重新排序功能。這使得模型能夠智慧地整合來自不同檢索源的結果，即使這些源的分數不可比較，例如同時融合傳統的 BM25 算法與神經檢索（如密集段落檢索，DPR）。透過引入重排序 (Reranker) 組件，模型得以統一處理並最大化初始候選池的質量。此外，Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 還引入了&lt;strong&gt;線上知識蒸餾方法&lt;/strong&gt;，實現了整個檢索與生成管道的端到端訓練。這種訓練方式將系統的所有組件串接成一個整體模型，直接以最終目標作為損失函數進行優化，從而無需分階段訓練，有效提升了系統的整體性能。&lt;/p&gt;
&lt;p&gt;※ 在機器學習領域，「端到端訓練」（End-to-End Training）指的是將整個系統的所有組件（如檢索、重排序、生成）串接為一個整體模型，直接以最終目標（如生成正確答案）作為損失函數，反向傳播優化所有參數。這種方式不需分階段分別訓練各模組，而是讓模型自動協調各部分，最大化整體性能。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id="基本架構介紹"&gt;基本架構介紹
&lt;/h1&gt;&lt;p&gt;RAG（Retrieval-Augmented Generation）與 Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G（Retrieve, Rerank, Generate）架構模組如下圖所示：&lt;br&gt;
&lt;img src="https://ar5iv.labs.arxiv.org/html/2207.06300/assets/x1.png"
loading="lazy"
alt="RAG 基本架構圖"
&gt;&lt;br&gt;
圖 1. RAG 基本架構圖：RAG 基礎流程包含檢索器與生成器，將查詢與檢索到的外部知識片段一同輸入生成模型，產生最終回答。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ar5iv.labs.arxiv.org/html/2207.06300/assets/x2.png"
loading="lazy"
alt="RAG 重排序架構圖"
&gt;&lt;br&gt;
圖 2. Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G (Retrieve, Rerank, Generate) 架構圖：進階版本在檢索後加入重排序（Rerank）模組，對候選片段進行排序優化，提升檢索結果品質與生成相關性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RAG 架構&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;檢索器（Retriever）：從語料庫中檢索相關段落&lt;/li&gt;
&lt;li&gt;生成器（Generator）：基於檢索結果生成回答&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 架構&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始檢索層（Initial Retrieval Layer）：神經檢索 + 關鍵字檢索&lt;/li&gt;
&lt;li&gt;重排序層（Reranker Layer）：融合多種檢索結果並重排序&lt;/li&gt;
&lt;li&gt;生成層（Generation Layer）：基於重排序結果生成最終輸出&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;這種分層設計使 Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 能靈活結合多種檢索技術，並透過重排序提升候選質量，最終增強生成效果。&lt;/p&gt;
&lt;h1 id="re2g-核心概念與創新突破"&gt;Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 核心概念與創新突破
&lt;/h1&gt;&lt;p&gt;Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 的兩大創新突破，分別體現在多源檢索結果的融合能力與創新的端到端訓練策略。
首先，Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 的重排序方法打破了傳統檢索分數不可比的限制，能夠有效融合來自不同檢索機制的候選結果。其次，作者提出一種改良的知識蒸餾策略，使得整個系統能僅依賴目標序列輸出進行訓練，實現從初始檢索、重排序到序列生成端到端改善。這項設計解決了當重排序器主導運算後，查詢編碼器將無法從生成損失中獲得有效梯度的問題。透過在線知識蒸餾，Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 讓重排序器成為查詢編碼器的指導，重新建立了梯度流通路，深化多模組系統間的互動訓練。&lt;/p&gt;
&lt;p&gt;主要流程是從大型語料庫中&lt;strong&gt;檢索相關段落&lt;/strong&gt;，隨後對這些初步檢索到的段落&lt;strong&gt;進行精確的重排序&lt;/strong&gt;，最後基於重排序後的結果&lt;strong&gt;生成最終的輸出序列&lt;/strong&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;檢索相關段落&lt;/strong&gt;&lt;br&gt;
從大型語料庫中，透過高效的初始檢索模型（如 DPR 或 BM25）篩選出與查詢相關的候選段落。&lt;br&gt;
&lt;img src="https://ar5iv.labs.arxiv.org/html/2207.06300/assets/x4.png"
loading="lazy"
alt="雙編碼器表示模型 — 初始檢索器（DPR）"
&gt;&lt;br&gt;
&lt;em&gt;圖 3：表示模型（Bi-Encoder）運作方式：查詢（Query）與段落（Passage）分別透過獨立的 BERT 編碼器轉換為向量表示（r&lt;!-- raw HTML omitted --&gt;q&lt;!-- raw HTML omitted --&gt; 與 r&lt;!-- raw HTML omitted --&gt;p&lt;!-- raw HTML omitted --&gt;），之後透過 向量內積（Inner Product） 計算相似度，作為檢索排序的依據。&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;精確的重排序&lt;/strong&gt;&lt;br&gt;
對初步檢索到的候選段落，使用互動模型（Reranker）進行相關性重排序。&lt;br&gt;
此階段的關鍵創新在於能夠&lt;strong&gt;整合多種來源的檢索結果&lt;/strong&gt;，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;BM25&lt;/strong&gt;：基於關鍵字的傳統檢索，分數反映字詞匹配頻率&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DPR&lt;/strong&gt;：基於語意向量的神經檢索，分數來自內積相似度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然而，這些檢索分數性質不同，直接比對會產生偏誤。&lt;br&gt;
為此，Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 引入 &lt;strong&gt;獨立的 Reranker 模組&lt;/strong&gt;，將不同檢索來源的候選進行&lt;strong&gt;統一評分&lt;/strong&gt;，重排序後產出可比較的結果。&lt;br&gt;
&lt;img src="https://ar5iv.labs.arxiv.org/html/2207.06300/assets/x3.png"
loading="lazy"
alt="互動模型 — 重排序器（Reranker）"
&gt;&lt;br&gt;
&lt;em&gt;圖 4：互動模型（Interaction Model）運作方式 — 查詢與段落聯合輸入 BERT，透過交叉注意力捕捉語義關聯，最終由 [CLS] 預測相關性分數。&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;基於重排序結果的生成&lt;/strong&gt;&lt;br&gt;
最終，將排序靠前的 Top-K 段落，與查詢共同輸入到序列生成模型（如 BART）中，生成目標回答或回應。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="re2g-模型架構詳解"&gt;Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 模型架構詳解
&lt;/h2&gt;&lt;h3 id="初始檢索層dpr與bm25的協同作用"&gt;初始檢索層：DPR與BM25的協同作用
&lt;/h3&gt;&lt;p&gt;目的是建立一個包含潛在相關段落的候選池，在此階段結合了兩種互補的檢索方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;DPR (Dense Passage Retrieval) 密集段落檢索：DPR 採用雙編碼器表示模型，其中查詢編碼器和段落編碼器（兩者均基於 BERT）獨立生成查詢和段落的表示向量。這種獨立編碼的設計允許在檢索前預先計算語料庫中所有段落的向量，並使用近似最近鄰 (ANN) 索引。在推斷時，輸入查詢會使用 DPR 查詢編碼器進行編碼，並從 HNSW 索引中快速檢索出最相關的 Top-12 段落。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BM25：作為一種傳統的基於關鍵字的檢索方法，BM25 擅長捕捉精確的詞彙匹配。在推斷時，查詢也會傳遞給 BM25 搜索（具體使用 Anserini 庫），並收集 Top-12 的 BM25 結果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 在此層結合使用了 DPR (一種神經「表示模型」) 和 BM25 (一種詞彙匹配方法)。這兩種方法互補，共同產生一個初步的候選段落池，這兩種方法檢索到的段落隨後會被合併，為後續的重排序階段提供一個更全面且多樣化的候選集。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;舉例：&lt;br&gt;
&lt;strong&gt;DPR 的檢索結果：&lt;/strong&gt;
DPR 模型會將文檔庫中的所有段落預先編碼成向量，並儲存在一個向量資料庫中，當有一個查詢進來時，DPR 會將這個查詢也編碼成一個向量，然後，它會在向量資料庫中進行「最近鄰搜索」，找出與查詢向量最相似的 K 個段落向量，這些被找出來的 K 個段落，就是 DPR 的「檢索結果」，它們是實際的文本段落。&lt;br&gt;
例如：「尼加拉瀑布位於加拿大和美國的邊界。」&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BM25 的檢索結果：&lt;/strong&gt;
BM25 演算法會根據查詢中的關鍵字，在文檔庫中計算每個段落與查詢的相關性分數，它會根據這些分數，找出相關性最高的 K 個段落，這些被找出來的 K 個段落，就是 BM25 的「檢索結果」，它們也是實際的文本段落。&lt;br&gt;
例如：「尼加拉瀑布是世界著名的瀑布。」&lt;/p&gt;
&lt;p&gt;合併指的是將這兩個獨立檢索器（DPR 和 BM25）各自找出來的候選段落列表結合起來，形成一個更大的、包含更多潛在相關段落的集合。&lt;/p&gt;
&lt;p&gt;例如：&lt;br&gt;
DPR 可能檢索到段落 A, B, C, D, E。&lt;br&gt;
BM25 可能檢索到段落 C, F, G, H, I。&lt;br&gt;
合併後，初始候選集就可能包含 A, B, C, D, E, F, G, H, I。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id="重排序層互動模型的強大能力"&gt;重排序層：互動模型的強大能力
&lt;/h3&gt;&lt;p&gt;重排序層的核心目的是精煉初始檢索結果的排名，並實現來自不同檢索方法結果的合併。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型類型&lt;/strong&gt;：Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 使用基於 Nogueira 和 Cho 序列對分類方法的「互動模型」進行重排序。&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;機制&lt;/strong&gt;： 在互動模型中，查詢和段落會作為一個整體輸入到 BERT 變換器中，模型會在兩個序列的所有詞元上共同應用交叉注意力機制，從而捕捉查詢和段落之間更深層次的交互關係。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;優勢&lt;/strong&gt;：透過使用互動模型對來自表示模型的 Top-N 段落進行重排序，Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 能夠同時獲得兩種模型類型的優勢：互動模型帶來的更高準確性（因其能進行更細緻的交叉注意力計算），以及表示模型帶來的可擴展性（因其允許高效的初始檢索）。這種設計模式在許多大規模資訊檢索系統中非常有效，平衡了對大型語料庫的快速初始檢索與對較小候選集的精細排名。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;初始化與推斷&lt;/strong&gt;： 重排序器從 NBoost 在 MS MARCO 數據集上訓練的 BERT 模型初始化。在推斷時，從初始檢索層合併後的段落集會傳遞給重排序器進行評分，並選出 Top-5 的段落供生成器使用。  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="生成層基於bart的序列生成"&gt;生成層：基於BART的序列生成
&lt;/h3&gt;&lt;p&gt;生成層負責根據重排序後的段落和查詢生成最終的目標輸出序列。  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;基礎模型&lt;/strong&gt;：Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 使用基於 BART 的序列到序列生成模型，具體是 $BART_{LARGE}$。BART 結合了雙向編碼器（如 BERT）和自回歸解碼器（如 GPT）的優勢，使其非常適合序列到序列任務，並能有效地整合檢索到的資訊。這種選擇利用了現有的穩健生成模型，並透過外部知識增強了它們的能力。  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;生成器輸入與輸出&lt;/strong&gt;：來自重排序器的 Top-5 段落會與原始查詢結合，作為 $BART_{LARGE}$ 的輸入以生成輸出。BART 生成的五個輸出序列隨後會根據重排序器分數的 softmax 進行加權，以產生最終的輸出。這個過程在 RAG 中稱為邊緣化 (marginalization)，它確保了檢索到的相關性信息能夠有效指導最終的生成。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id="四階段訓練方法與線上知識蒸餾"&gt;四階段訓練方法與線上知識蒸餾
&lt;/h2&gt;&lt;h3 id="第一階段dpr-訓練"&gt;第一階段：DPR 訓練
&lt;/h3&gt;&lt;p&gt;訓練數據由查詢、正向段落（來自真實標籤的來源資訊）以及「硬負向」段落（從 BM25 檢索但非真實標籤的段落）的三元組組成，這些實例會被批次處理，並且批次中其他實例的正向和硬負向段落會被用作當前實例的「批次負向」，DPR 雙編碼器模型隨後會為每個查詢提供其對正向、硬負向和批次負向段落的概率分佈。此階段的損失函數是正向段落的負對數似然，完成此階段訓練後，語料庫中的所有段落都會使用分層可導航小世界圖 (HNSW) 結合 FAISS 庫進行索引，以便後續高效檢索。&lt;/p&gt;
&lt;h3 id="第二階段生成訓練"&gt;第二階段：生成訓練
&lt;/h3&gt;&lt;p&gt;此階段的訓練目標是擴展查詢編碼器的訓練，並訓練 BART&lt;!-- raw HTML omitted --&gt;LARGE&lt;!-- raw HTML omitted --&gt; 序列到序列模型以生成最終的目標序列輸出。這個訓練過程與 Lewis 等人描述的 RAG 模型生成訓練方法一致。&lt;/p&gt;
&lt;h3 id="第三階段重排序訓練"&gt;第三階段：重排序訓練
&lt;/h3&gt;&lt;p&gt;重排序器的獨立訓練始於收集訓練集上來自 DPR 和 BM25 的初始檢索結果，這些結果隨後會被合併，並作為重排序器的訓練數據。由於某些數據集可能存在多個正向段落，因此此階段採用的損失函數是這些正向段落負對數似然之和。&lt;/p&gt;
&lt;h3 id="第四階段端到端訓練"&gt;第四階段：端到端訓練
&lt;/h3&gt;&lt;p&gt;端到端訓練帶來了一個特殊挑戰：在 Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 中，重排序器的分數而非初始檢索分數用於加權每個序列在生成中的影響。這意味著重排序器可以直接透過目標輸出的真實標籤進行訓練，但查詢編碼器的梯度將為零，因為邊緣化過程不再直接依賴於查詢和段落表示向量的內積。  &lt;/p&gt;
&lt;p&gt;為了解決這個問題，Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 引入了一種新穎的線上知識蒸餾應用。在這種方法中，重排序器作為「教師模型」，為 DPR「學生模型」提供軟標籤。這種知識蒸餾是「線上」發生的，即在重排序器訓練的同時進行，實現了知識從一種架構（互動模型）到另一種架構（表示模型）的轉移。初始檢索（DPR）的損失函數是其在檢索到的段落上給出的概率分佈與重排序器在相同段落上給出的概率分佈之間的 KL 散度。為了平滑這些分佈、防止過度損失並穩定訓練，模型使用了溫度超參數 (T)。這種方法不僅提供了正向和負向實例的信號，還提供了「負向程度」的信號，並且能夠利用更多段落進行訓練（DPR 檢索 12 個段落，而生成只使用 Top-5），從而提供了比二元標籤更豐富的訓練信號。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="實驗結果與性能分析"&gt;實驗結果與性能分析
&lt;/h2&gt;&lt;p&gt;此部分尚未撰寫。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="結論"&gt;結論
&lt;/h2&gt;&lt;p&gt;Re&lt;!-- raw HTML omitted --&gt;2&lt;!-- raw HTML omitted --&gt;G 在槽填充、問答、事實核查和對話等任務中，無論是檢索還是端到端性能都得到了實質性提升。&lt;/p&gt;
&lt;p&gt;本研究的關鍵成果包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;重排序器的有效性。  &lt;/li&gt;
&lt;li&gt;線上知識蒸餾的成功。  &lt;/li&gt;
&lt;li&gt;多源檢索的益處。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="reference"&gt;Reference
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class="link" href="https://ar5iv.labs.arxiv.org/html/2207.06300" target="_blank" rel="noopener"
&gt;Re2G: Retrieve, Rerank, Generate-ar5iv 可視化版本&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>檢索增強大型語言模型綜述 — RA-LLMs 系統性回顧與應用解析</title><link>https://Dandelionlibra.github.io/post/paper/rag-llms-survey-note/</link><pubDate>Sun, 20 Jul 2025 08:50:00 +0800</pubDate><guid>https://Dandelionlibra.github.io/post/paper/rag-llms-survey-note/</guid><description>&lt;blockquote&gt;
&lt;p&gt;本文整理自：&lt;a class="link" href="https://arxiv.org/abs/2405.06211" target="_blank" rel="noopener"
&gt;A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models&lt;/a&gt;&lt;br&gt;
作者：Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li&lt;br&gt;
發佈於 arXiv，2024年5月&lt;/p&gt;&lt;/blockquote&gt;
&lt;h1 id="摘要"&gt;摘要
&lt;/h1&gt;&lt;p&gt;大型語言模型（LLMs）雖展現強大生成能力，但受限於內部知識與幻覺問題。檢索增強生成（Retrieval-Augmented Generation;RAG）透過即時檢索外部資訊，提升回應的可靠性與時效性。本文整理 RA-LLMs 的架構、訓練策略與應用，並探討其面臨的挑戰與未來發展，展現檢索對提升 LLM 實用性的關鍵價值。&lt;/p&gt;
&lt;h1 id="前言"&gt;前言
&lt;/h1&gt;&lt;p&gt;檢索增強生成（RAG）技術透過結合資訊檢索與大型語言模型（LLMs），補足模型知識不足與幻覺問題，近年受到廣泛關注。LLMs 雖具強大生成能力，卻常受限於知識時效與專領域應用，而 RA-LLMs 則透過檢索外部資料提升生成品質。&lt;/p&gt;
&lt;h1 id="背景"&gt;背景
&lt;/h1&gt;&lt;h2 id="大型語言模型"&gt;大型語言模型
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;應用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在特定資料集上進行微調，LLM 可以適應各種下游任務，使其能夠專注於特定領域或應用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;架構:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Encoder-only 模型，雙向編碼，可同時考慮單詞左右語境。&lt;br&gt;
ex. BERT&lt;/li&gt;
&lt;li&gt;Decoder-only 模型，單向生成（左至右），根據前文預測下個字元。&lt;br&gt;
ex. GPT&lt;/li&gt;
&lt;li&gt;Encoder-Decoder 模型，將輸入編碼後，再由解碼器生成對應輸出。&lt;br&gt;
ex. T5&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="提示學習"&gt;提示學習
&lt;/h2&gt;&lt;h3 id="提示工程prompt-engineering"&gt;提示工程（Prompt Engineering）
&lt;/h3&gt;&lt;p&gt;因為 LLM 的參數量通常非常龐大，因此提示學習的發展可使 LLM 不需為了特定任務進行大量微調，就可以實現各項任務。&lt;br&gt;
缺點：當缺乏專業領域知識時，生成結果可能不夠精確。&lt;/p&gt;
&lt;h3 id="上下文學習in-context-learning-icl"&gt;上下文學習（In-Context Learning, ICL）
&lt;/h3&gt;&lt;p&gt;為提示學習的一種形式，透過在提示中提供範例示範，讓 LLM 觀察並學習任務模式。&lt;br&gt;
缺點：成效高度依賴範例品質、當缺乏必要知識或資訊時，可能導致生成結果不理想。&lt;/p&gt;
&lt;p&gt;為克服這些問題，發展出 RAG（檢索增強生成）技術，RAG 結合檢索與生成，提升 LLM 在多任務中的表現與適應性。&lt;/p&gt;
&lt;h1 id="內文"&gt;內文
&lt;/h1&gt;&lt;p&gt;LLMs 時代的 RAG 架構大致包含檢索、產生和增強三個主要流程。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ar5iv.labs.arxiv.org/html/2405.06211/assets/x2.png"
loading="lazy"
alt="RAG 系統總覽"
&gt;&lt;br&gt;
&lt;em&gt;圖 1：RAG 系統總覽，涵蓋檢索、生成與增強三大流程。來源：原論文&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ar5iv.labs.arxiv.org/html/2405.06211/assets/x3.png"
loading="lazy"
alt="RAG 系統細節流程"
&gt;&lt;br&gt;
&lt;em&gt;圖 2：RAG 在 RA-LLM 中的流程圖，展示各模組間的互動。來源：原論文&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="retrieval"&gt;Retrieval
&lt;/h2&gt;&lt;p&gt;RAG 旨在從外部知識源提供關鍵訊息給 LLM。&lt;/p&gt;
&lt;h3 id="retriever-type"&gt;Retriever Type
&lt;/h3&gt;&lt;p&gt;依照資訊編碼區分。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;稀疏檢索（Sparse Retrieval）：直接匹配詞彙並依頻率排名。&lt;/li&gt;
&lt;li&gt;稠密檢索（Dense Retrieval）：將查詢與文檔嵌入為向量，透過語意相似度檢索。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="retrieval-granularity"&gt;Retrieval Granularity
&lt;/h3&gt;&lt;p&gt;檢索單位的選擇對效能與計算成本有重大影響。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chunk(passages): 包含緊湊且完整的資訊，冗餘和不相關性較少，是 RAG 中主流的檢索文本粒度。&lt;/li&gt;
&lt;li&gt;Token: 實現更快的搜尋，但會給資料庫儲存帶來更多負擔。適用於需要稀有模式或領域外資料的情況。&lt;/li&gt;
&lt;li&gt;Entity: 實體檢索是從知識而非語言的角度檢索，對於以實體為中心的任務更有效，並且與詞元檢索相比，在空間上更高效。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="retrieval-enhancement-strategies"&gt;Retrieval Enhancement strategies
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;檢索前優化（Pre-retrieval）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Query Expansion (查詢擴展)：透過加入相關詞彙或概念來擴大原始查詢的範圍。例如，利用大型語言模型 (LLM) 生成偽文件，並從中提取相關資訊來擴展查詢，有助於消除歧義並引導檢索器。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Query Rewrite (查詢重寫)：旨在重新彌合原始查詢，使其更適合檢索。這可能涉及澄清查詢意圖、使其更精確，或是將其轉換為檢索功能更容易理解的形式。例如，利用 LLM 將原始問題重寫為更利於檢索的版本。&lt;br&gt;
舉例，&lt;strong&gt;多次&lt;/strong&gt;詢問模型他的檢索資料是否正確。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Query Augmentation (查詢增強)：將原始查詢與初步生成的內容結合，形成一個新的查詢。這種策略可以增加查詢與潛在相關文件之間的詞彙和語義重疊，有助於檢索出更多有助於答案生成的資訊。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;檢索後優化（Post-retrieval）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;重排序與過濾&lt;br&gt;
對檢索到的文件進行重新排序，將最相關的資訊排在前面，並過濾掉不相關或低品質的文件。例如，透過不同的檢索方法組裝文件並進行重排序，以提升檢索結果的穩健性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;雜訊過濾與整合&lt;br&gt;
處理檢索到的資訊中可能存在的雜訊或不相關內容，以避免其對生成過程產生負面影響。同時，將清洗過的資訊有效地整合進生成模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;壓縮與摘要&lt;br&gt;
針對檢索到的長篇文件，進行壓縮或生成摘要，以解決大型語言模型輸入長度限制的問題。例如，將檢索到的文件處理成文本摘要，再用於模型生成。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="database"&gt;Database
&lt;/h3&gt;&lt;p&gt;RA-LLM 的檢索資料庫可為封閉式或開放式來源。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;封閉式資料庫:
通常以鍵值對 (key-value pairs) 的形式儲存知識。&lt;/li&gt;
&lt;li&gt;開放式資料庫:
利用搜尋引擎（如 Bing、Google）獲取即時資訊。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="生成generation"&gt;生成（Generation）
&lt;/h2&gt;&lt;p&gt;生成模組的設計高度依賴於下游任務需求，因而得以適應不同的任務需求。&lt;/p&gt;
&lt;h3 id="可調參數生成器白箱parameter-accessible-generators"&gt;可調參數生成器（白箱，Parameter-Accessible Generators）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Encoder-Decoder&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;擁有獨立的編碼器（Encoder）與解碼器（Decoder），分別處理輸入與生成的目標。&lt;/li&gt;
&lt;li&gt;Encoder 先將輸入編碼為上下文表示，Decoder 以 Cross-Attention 讀取 Encoder 輸出，逐步生成結果。&lt;/li&gt;
&lt;li&gt;模型的目標是「根據編碼後的輸入與先前生成的結果，預測下一個 token」。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;輸入：請介紹 Transformer。
Encoder 編碼後 → [內部上下文表示]
Decoder 讀取表示 → 生成：Transformer 是一種...
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decoder-only&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;沒有獨立的編碼器。&lt;/li&gt;
&lt;li&gt;輸入（如問題、提示） 和 目標（要生成的內容） 會被串接成同一序列，並從左到右進行處理。&lt;/li&gt;
&lt;li&gt;模型的目標是學會「根據前面內容，預測下一個 token」。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;輸入：請介紹 Transformer。
模型看到的內容：請介紹 Transformer。&amp;lt;接著是生成的回答...&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="不可調參數生成器黑箱parameter-inaccessible-generators"&gt;不可調參數生成器（黑箱，Parameter-Inaccessible Generators）
&lt;/h3&gt;&lt;p&gt;無法直接修改模型本身，且難以進行微調，因此更側重於優化檢索和增強的過程。它們的目標是透過為輸入 (prompts) 提供更優質的知識、指導或範例來增強 Generator 的性能。&lt;/p&gt;
&lt;h2 id="增強augmentation"&gt;增強（Augmentation）
&lt;/h2&gt;&lt;h3 id="input-layer-integration"&gt;Input-Layer Integration:
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;串聯整合&lt;/strong&gt;：如 In-Context RALM (Ram et al., 2023)，將原始輸入與所有檢索文件串聯為單一序列輸入生成模型。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;問題：輸入長度易超過模型處理上限，需移除部分詞元，可能導致資訊遺失。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;平行整合&lt;/strong&gt;：如 FID (Izacard and Grave, 2021b)、Atlas、REPLUG，將每個檢索文件獨立編碼，僅在後續步驟聚合結果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;優點：更能擴展至大量上下文，減少資訊丟失風險。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通常，大多數基於黑盒生成的 RAG 方法都採用此法，因為生成模型的中間層和輸出分佈都無法存取。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="output-layer-integration"&gt;Output-Layer Integration:
&lt;/h3&gt;&lt;p&gt;一種後處理 (post-hoc) 的檢索增強方式，它不直接干預生成模型的內部運作或其生成過程，而是在模型產生初步結果之後，才將檢索到的資訊與這些結果進行結合。&lt;/p&gt;
&lt;h3 id="intermediate-layer-integration"&gt;Intermediate-Layer Integration:
&lt;/h3&gt;&lt;p&gt;在生成模型內部的中間層注入檢索資訊，相較於輸入層與輸出層整合，屬於 半參數式（Semi-parametric） 的強化方式，具有更高的資訊融合深度與潛力。&lt;/p&gt;
&lt;h3 id="34retrieval-augmentation-necessity-and-frequency"&gt;3.4.Retrieval Augmentation Necessity and Frequency
&lt;/h3&gt;&lt;p&gt;基於 LLM 的生成中，檢索操作旨在補充知識以增強生成。儘管檢索增強模型前景光明，但若不加區分地使用不相關的段落進行增強，可能會覆蓋 LLM 已有的正確知識，導致錯誤回應，甚至使幻覺率翻倍。因此，對於檢索增強型 LLM (RA-LLMs) 來說，&lt;strong&gt;準確回憶先驗知識並僅在必要時選擇性地整合檢索資訊&lt;/strong&gt;至關重要，這是實現穩健 RA-LLMs 的關鍵。&lt;/p&gt;
&lt;h4 id="檢索必要性判斷"&gt;檢索必要性判斷
&lt;/h4&gt;&lt;p&gt;大多數方法基於 LLM 的初步答案或內部推理結果來判斷是否需要檢索：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;特殊標記控制&lt;/strong&gt;：如 Self-RAG，引入特殊標記評估檢索必要性並控制行為。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;迭代提示決策&lt;/strong&gt;：設計迭代提示決定生成中是否需要額外資訊。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基於信賴度 (Logits Confidence)&lt;/strong&gt;：傳統 RAG 中透過評估生成模型輸出的 logits 信賴度來判斷。如 FLARE，當 logits 低於閾值時動態觸發 RAG。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;協同檢測&lt;/strong&gt;：如 SlimPLM，利用輕量級代理模型生成「啟發式答案」檢測 LLM 缺失知識，並用於查詢重寫以促進檢索。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="檢索頻率-retrieval-frequency"&gt;檢索頻率 (Retrieval Frequency)
&lt;/h4&gt;&lt;p&gt;檢索頻率（或稱檢索步長）是決定生成過程中檢索使用程度的重要設計考量，影響模型的效率和有效性。當不考慮檢索必要性時，檢索頻率通常是預定義和固定的，主要有三種設定：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;一次性檢索 (One-time retrieval)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;方式&lt;/strong&gt;：在生成過程開始時只調用一次檢索功能，檢索所有所需資訊，然後提供給生成模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;適用場景&lt;/strong&gt;：外部資料庫資訊需求對 LLM 來說很明確的情況。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;限制&lt;/strong&gt;：對於需要長篇輸出的任務（如開放域摘要），預先檢索的文件可能不足以支持整個生成序列，需要生成中進行檢索操作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;範例&lt;/strong&gt;：REALM。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;每 N 個詞元檢索 (Every-n-token retrieval)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;方式&lt;/strong&gt;：在生成過程中每隔 N 個詞元觸發一次檢索。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;適用場景&lt;/strong&gt;：需要持續資訊補充的長篇生成任務。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;範例&lt;/strong&gt;：In-Context RALM、RETRO。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;每個詞元檢索 (Every-token retrieval)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;方式&lt;/strong&gt;：在生成過程中，為每個詞元的預測都檢索資訊。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;頻率&lt;/strong&gt;：最頻繁的檢索策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;範例&lt;/strong&gt;：kNN-LM。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;權衡&lt;/strong&gt;：
總體而言，檢索頻率影響 RAG 方法的有效性和效率。更頻繁的檢索通常帶來更好的性能，但也顯著增加計算成本。&lt;/p&gt;
&lt;h2 id="ra-llms-訓練策略概述"&gt;RA-LLMs 訓練策略概述
&lt;/h2&gt;&lt;p&gt;&lt;img src="https://ar5iv.labs.arxiv.org/html/2405.06211/assets/x4.png"
loading="lazy"
alt="RA-LLMs 訓練策略總覽"
&gt;&lt;br&gt;
&lt;em&gt;圖 3：RA-LLMs 訓練策略總覽，涵蓋免訓練與需訓練方法。來源：原論文&lt;/em&gt;&lt;/p&gt;
&lt;h3 id="免訓練方法training-free"&gt;免訓練方法（Training-Free）
&lt;/h3&gt;&lt;h4 id="prompt-engineering-based-methods"&gt;Prompt Engineering-based Methods
&lt;/h4&gt;&lt;p&gt;將檢索到的外部知識，直接整合進 LLM 的提示（Prompt），作為上下文輔助模型生成。&lt;br&gt;
舉例，In-Context RALM 在不改動 LLM 參數的情況下，直接將檢索到的文件插入於原始提示之前，增強生成過程。IRCoT 則交錯進行 chain-of-thought（CoT）生成與知識檢索步驟，使每一步推理都能檢索到更相關的資訊。GENREAD 不是從大型語料庫檢索知識，而是先讓 LLM 根據查詢生成上下文文件，再根據這些上下文與問題產生答案。SKR 則引導 LLM 判斷是否能僅依靠內部知識回答問題，若不足再選擇性調用檢索器，靈活結合內外部知識。TOC 針對模糊問題，先檢索相關知識，並遞迴將問題拆解為多個明確子問題，最終聚合生成長篇答案。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特點：
&lt;ul&gt;
&lt;li&gt;無需模型訓練&lt;/li&gt;
&lt;li&gt;靠設計合理的提示與檢索流程提升效果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="retrieval-guided-token-generation-methods"&gt;Retrieval-Guided Token Generation Methods
&lt;/h4&gt;&lt;p&gt;透過檢索結果來調整 LLM 的 Token 預測分布，影響每一步的生成。&lt;br&gt;
舉例，例如 KNN-LMs 會根據當前查詢從資料庫檢索出 k 個最相關的上下文，計算鄰近分布，並將其與原模型的輸出分布進行插值校正，以提升生成結果的準確性。Rest 則以非參數檢索資料庫取代傳統的參數式草稿模型，根據當前上下文檢索相關 token，輔助推測式生成（speculative decoding）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特點：
&lt;ul&gt;
&lt;li&gt;不更改模型權重&lt;/li&gt;
&lt;li&gt;通常作為後處理或推測性生成的輔助&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;※ 這兩類免訓練方法，分別著重於：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提示工程 — 調整輸入&lt;/li&gt;
&lt;li&gt;生成控制 — 調整輸出過程&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id="需訓練方法training-based"&gt;需訓練方法（Training-Based）
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Independent Training&lt;br&gt;
獨立訓練方法會將 RAG 流程中的每個組件分開、獨立地進行訓練，這意味著在訓練過程中，這兩個組件之間沒有任何交互作用。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;目的與優勢：&lt;/strong&gt;&lt;br&gt;
相較於無需訓練的方法，獨立訓練能有效提升 RAG 模型的性能。&lt;br&gt;
1. 訓練 LLMs 以更好地利用檢索到的知識。&lt;br&gt;
2. 訓練檢索器以彌合資訊檢索與語言生成之間的差距。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;檢索器類型：&lt;/strong&gt;
* 稀疏檢索器 (Sparse Retriever) ：
這類檢索器通常利用稀疏特徵，例如詞頻，來表示文件，並根據任務特定的指標（如 TF-IDF 和 BM25）計算相關性分數 。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;* 密集檢索器 (Dense Retriever) ：
密集檢索器則採用深度神經網絡將查詢和文件編碼成密集表示 (dense representations)。然後，通常使用內積計算相關性分數並檢索相關的外部知識。序列訓練透過協調訓練的方式，尋求更深層次的整合效果。
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;p&gt;Sequential Training&lt;br&gt;
序列訓練方法則採取分階段的訓練方式。首先訓練一個模組（例如檢索器），然後再利用這個訓練好的模組去指導另一個模組（例如生成器）的調整過程，目的在改善模組間的協同作用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;訓練流程：&lt;/strong&gt; 序列訓練通常分為兩個階段&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;**初始預訓練：**首先對檢索器或生成器中的一個模組進行獨立的預訓練。&lt;/li&gt;
&lt;li&gt;**固定與訓練：**一旦其中一個模組完成預訓練，它就會被固定（freeze）下來，而另一個模組則在其輔助下進行訓練。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;優勢與靈活性：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;協同增益：與獨立訓練相比，序列訓練的優勢在於可訓練的模組能夠受益於固定模組的引導和協助，從而更好地適應彼此。&lt;/li&gt;
&lt;li&gt;利用現有模型：值得注意的是，許多已經預訓練好的強大模型（例如 BERT、CLIP、T5）可以直接作為固定模組使用，從而省略了初始的預訓練步驟，進一步提高了效率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;根據檢索器和生成器之間的訓練順序，序列訓練可分為兩大類：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;檢索器優先 (Retriever First)：&lt;br&gt;
此類方法首先訓練檢索器，然後將其固定，再訓練生成器。&lt;/li&gt;
&lt;li&gt;LLMs 優先 (LLMs First)：&lt;br&gt;
此類方法則相反，先訓練 LLM，再將其固定，然後訓練檢索器。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Joint Training&lt;br&gt;
聯合訓練方法則是同時訓練檢索器和生成器，這種方式是為了讓兩個模組在訓練過程中相互協調、共同進步。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description></item></channel></rss>